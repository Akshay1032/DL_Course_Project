{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6908a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c2e9963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fac019c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FlowerDataset class\n",
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_file, transform=None, pca_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.pca_transform = pca_transform\n",
    "        # Load image file names\n",
    "        self.image_filenames = os.listdir(image_dir)\n",
    "        print(f\"Loaded {len(self.image_filenames)} images from {image_dir}\")\n",
    "        # Load labels from the labels file\n",
    "        with open(labels_file, 'r') as f:\n",
    "            # Subtract 1 from each label to make them zero-indexed\n",
    "            self.labels = [int(line.strip()) - 1 for line in f.readlines()]  \n",
    "        print(f\"Loaded {len(self.labels)} labels from {labels_file}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.labels[idx]  # Labels should already be integers\n",
    "\n",
    "        # Apply PCA transform if it exists\n",
    "        if self.pca_transform is not None:\n",
    "            flattened_image = image.view(-1).numpy()  # Flatten the image\n",
    "            pca_image = self.pca_transform.transform(flattened_image.reshape(1, -1))  # Transform using PCA\n",
    "            return torch.tensor(pca_image, dtype=torch.float32).squeeze(0), label  # Return PCA-transformed image\n",
    "\n",
    "        return image, label  # Fallback to original image if PCA is not applied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dba18422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3000 images from train_data\n",
      "Loaded 3000 labels from train_labels.txt\n",
      "Loaded 3000 images from train_data\n",
      "Loaded 3000 labels from train_labels.txt\n",
      "Loaded 600 images from val_data\n",
      "Loaded 600 labels from val_labels.txt\n"
     ]
    }
   ],
   "source": [
    "# Paths to your image directory and labels file\n",
    "image_dir = 'train_data'\n",
    "labels_file = 'train_labels.txt'\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = FlowerDataset(image_dir=image_dir, labels_file=labels_file, transform=data_transforms)\n",
    "\n",
    "# Collect flattened images for PCA\n",
    "flattened_images = []\n",
    "for image, _ in train_dataset:\n",
    "    flattened_image = image.view(-1).numpy()  # Flatten and convert to numpy\n",
    "    flattened_images.append(flattened_image)\n",
    "\n",
    "# Apply PCAb\n",
    "n_components = 100  # Adjust the number of components as needed\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(flattened_images)\n",
    "\n",
    "# Now create the DataLoader with PCA transform\n",
    "train_dataset = FlowerDataset(image_dir=image_dir, labels_file=labels_file, transform=data_transforms, pca_transform=pca)\n",
    "\n",
    "# Create the DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Load validation dataset\n",
    "val_image_dir = 'val_data'\n",
    "val_labels_file = 'val_labels.txt'\n",
    "val_dataset = FlowerDataset(image_dir=val_image_dir, labels_file=val_labels_file, transform=data_transforms, pca_transform=pca)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba9cb6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of augmented images considered for training: 3000\n"
     ]
    }
   ],
   "source": [
    "# Check number of images considered for training after augmentation\n",
    "augmented_images_count = 0\n",
    "\n",
    "# Loop through the DataLoader to count the total images generated\n",
    "for data, _ in train_loader:\n",
    "    augmented_images_count += data.size(0)  # Count the batch size\n",
    "\n",
    "print(f'Total number of augmented images considered for training: {augmented_images_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1042f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FlowerNet model\n",
    "class FlowerNet(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):  # Corrected method name\n",
    "        super(FlowerNet, self).__init__()  # Corrected method name\n",
    "        self.fc1 = nn.Linear(num_features, 512)  # Input layer\n",
    "        self.fc2 = nn.Linear(512, 256)            # Hidden layer\n",
    "        self.fc3 = nn.Linear(256, num_classes)    # Output layer\n",
    "        self.dropout = nn.Dropout(0.3)             # Dropout layer for regularization\n",
    "        self.norm = nn.LayerNorm(512)              # Layer normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))                   # First layer with ReLU activation\n",
    "        x = self.norm(x)                          # Normalization after activation\n",
    "        x = F.relu(self.fc2(x))                   # Second layer with ReLU activation\n",
    "        x = self.dropout(x)                       # Apply dropout\n",
    "        x = self.fc3(x)                           # Output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3da0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of features and classes\n",
    "num_features = 100  # Adjust this based on the number of PCA components\n",
    "num_classes = 60\n",
    "model = FlowerNet(num_features, num_classes)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_epochs = 30\n",
    "\n",
    "# Set up criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a2164a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum label: 0\n",
      "Maximum label: 59\n"
     ]
    }
   ],
   "source": [
    "# Added this\n",
    "def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    if self.transform:\n",
    "        image = self.transform(image)\n",
    "    \n",
    "    label = self.labels[idx]  # Labels should already be integers\n",
    "\n",
    "    print(f\"Label for image {idx}: {label}\")  # Debugging line\n",
    "\n",
    "    # Apply PCA transform if it exists\n",
    "    if self.pca_transform is not None:\n",
    "        flattened_image = image.view(-1).numpy()  # Flatten the image\n",
    "        pca_image = self.pca_transform.transform(flattened_image.reshape(1, -1))  # Transform using PCA\n",
    "        return torch.tensor(pca_image, dtype=torch.float32).squeeze(0), label  # Return PCA-transformed image\n",
    "\n",
    "    return image, label  # Fallback to original image if PCA is not applied\n",
    "print(\"Minimum label:\", min(train_dataset.labels))\n",
    "print(\"Maximum label:\", max(train_dataset.labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8ce8b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label range in training dataset: min=0, max=59\n",
      "Training data size: 3000\n",
      "Sample from train loader: [tensor([[-62.1666, -51.9524,   0.7376,  ...,  -2.0148,   0.1644,  -2.1931],\n",
      "        [-30.6468,  -2.0472,  82.2818,  ...,   3.1265,   2.2207,   3.1108],\n",
      "        [ 64.6472, -36.2351,  16.2443,  ...,   2.1767,  -6.4202,  -1.6366],\n",
      "        ...,\n",
      "        [ 18.8139,  -1.4905,  -2.9047,  ...,   2.9370,  -0.3992,   0.5593],\n",
      "        [  2.9876, -27.3472, -17.2954,  ...,  -0.7747,  -0.9865,   4.0756],\n",
      "        [ 79.6175, -37.8105,  25.8011,  ...,   2.6600,   2.2295,   0.7990]]), tensor([17, 52, 52, 29, 13, 13, 28, 59, 16, 11, 52, 22,  2, 19, 32, 37, 40, 34,\n",
      "        42, 30, 38, 45, 37, 39, 35, 24, 59,  7,  0, 19, 37, 11])]\n"
     ]
    }
   ],
   "source": [
    "#Added this\n",
    "print(f\"Label range in training dataset: min={min(train_dataset.labels)}, max={max(train_dataset.labels)}\")\n",
    "print(f'Training data size: {len(train_loader.dataset)}')\n",
    "print(f'Sample from train loader: {next(iter(train_loader))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58206799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/1\n",
      "Processing batch 1/94\n",
      "Processing batch 2/94\n",
      "Processing batch 3/94\n",
      "Processing batch 4/94\n",
      "Processing batch 5/94\n",
      "Processing batch 6/94\n",
      "Processing batch 7/94\n",
      "Processing batch 8/94\n",
      "Processing batch 9/94\n",
      "Processing batch 10/94\n",
      "Processing batch 11/94\n",
      "Processing batch 12/94\n",
      "Processing batch 13/94\n",
      "Processing batch 14/94\n",
      "Processing batch 15/94\n",
      "Processing batch 16/94\n",
      "Processing batch 17/94\n",
      "Processing batch 18/94\n",
      "Processing batch 19/94\n",
      "Processing batch 20/94\n",
      "Processing batch 21/94\n",
      "Processing batch 22/94\n",
      "Processing batch 23/94\n",
      "Processing batch 24/94\n",
      "Processing batch 25/94\n",
      "Processing batch 26/94\n",
      "Processing batch 27/94\n",
      "Processing batch 28/94\n",
      "Processing batch 29/94\n",
      "Processing batch 30/94\n",
      "Processing batch 31/94\n",
      "Processing batch 32/94\n",
      "Processing batch 33/94\n",
      "Processing batch 34/94\n",
      "Processing batch 35/94\n",
      "Processing batch 36/94\n",
      "Processing batch 37/94\n",
      "Processing batch 38/94\n",
      "Processing batch 39/94\n",
      "Processing batch 40/94\n",
      "Processing batch 41/94\n",
      "Processing batch 42/94\n",
      "Processing batch 43/94\n",
      "Processing batch 44/94\n",
      "Processing batch 45/94\n",
      "Processing batch 46/94\n",
      "Processing batch 47/94\n",
      "Processing batch 48/94\n",
      "Processing batch 49/94\n",
      "Processing batch 50/94\n",
      "Processing batch 51/94\n",
      "Processing batch 52/94\n",
      "Processing batch 53/94\n",
      "Processing batch 54/94\n",
      "Processing batch 55/94\n",
      "Processing batch 56/94\n",
      "Processing batch 57/94\n",
      "Processing batch 58/94\n",
      "Processing batch 59/94\n",
      "Processing batch 60/94\n",
      "Processing batch 61/94\n",
      "Processing batch 62/94\n",
      "Processing batch 63/94\n",
      "Processing batch 64/94\n",
      "Processing batch 65/94\n",
      "Processing batch 66/94\n",
      "Processing batch 67/94\n",
      "Processing batch 68/94\n",
      "Processing batch 69/94\n",
      "Processing batch 70/94\n",
      "Processing batch 71/94\n",
      "Processing batch 72/94\n",
      "Processing batch 73/94\n",
      "Processing batch 74/94\n",
      "Processing batch 75/94\n",
      "Processing batch 76/94\n",
      "Processing batch 77/94\n",
      "Processing batch 78/94\n",
      "Processing batch 79/94\n",
      "Processing batch 80/94\n",
      "Processing batch 81/94\n",
      "Processing batch 82/94\n",
      "Processing batch 83/94\n",
      "Processing batch 84/94\n",
      "Processing batch 85/94\n",
      "Processing batch 86/94\n",
      "Processing batch 87/94\n",
      "Processing batch 88/94\n",
      "Processing batch 89/94\n",
      "Processing batch 90/94\n",
      "Processing batch 91/94\n",
      "Processing batch 92/94\n",
      "Processing batch 93/94\n",
      "Processing batch 94/94\n",
      "Epoch: 1 \tTraining Loss: 4.099791 \tTraining Accuracy: 1.60%\n",
      "Validating batch 1/19\n",
      "Validating batch 2/19\n",
      "Validating batch 3/19\n",
      "Validating batch 4/19\n",
      "Validating batch 5/19\n",
      "Validating batch 6/19\n",
      "Validating batch 7/19\n",
      "Validating batch 8/19\n",
      "Validating batch 9/19\n",
      "Validating batch 10/19\n",
      "Validating batch 11/19\n",
      "Validating batch 12/19\n",
      "Validating batch 13/19\n",
      "Validating batch 14/19\n",
      "Validating batch 15/19\n",
      "Validating batch 16/19\n",
      "Validating batch 17/19\n",
      "Validating batch 18/19\n",
      "Validating batch 19/19\n",
      "Validation Loss: 4.094574, Validation Accuracy: 1.67%\n"
     ]
    }
   ],
   "source": [
    "# Make sure to define the device properly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move model to the selected device\n",
    "\n",
    "# Training Loop\n",
    "n_epochs = 1\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Starting Epoch {epoch + 1}/{n_epochs}\")\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        print(f\"Processing batch {batch_idx + 1}/{len(train_loader)}\")  # Debugging print\n",
    "        # Move data and target to the same device as the model\n",
    "        data = data.to(device)  \n",
    "        target = target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        output = model(data)   # Forward pass\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)  \n",
    "        loss.backward()        # Backward pass\n",
    "        optimizer.step()       # Optimize parameters\n",
    "        \n",
    "        train_loss += loss.item() * data.size(0)  # Accumulate training loss\n",
    "        pred = output.argmax(dim=1)  # Get predictions\n",
    "        correct += pred.eq(target).sum().item()  # Count correct predictions\n",
    "\n",
    "    # Average training loss and accuracy\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_accuracy = correct / len(train_loader.dataset)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1} \\tTraining Loss: {train_loss:.6f} \\tTraining Accuracy: {100. * train_accuracy:.2f}%')\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():  # No gradient calculation during validation\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            print(f\"Validating batch {batch_idx + 1}/{len(val_loader)}\")  # Debugging print\n",
    "            data = data.to(device)  # Move data to the same device\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(data)  # Forward pass\n",
    "            loss = criterion(output, target)  # Compute loss\n",
    "            val_loss += loss.item() * data.size(0)  # Accumulate validation loss\n",
    "            pred = output.argmax(dim=1)  # Get predictions\n",
    "            correct += pred.eq(target).sum().item()  # Count correct predictions\n",
    "\n",
    "    # Average validation loss and accuracy\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct / len(val_loader.dataset)\n",
    "\n",
    "    print(f'Validation Loss: {val_loss:.6f}, Validation Accuracy: {100. * val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "364e4cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'flower_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
