{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJkk4OrtJwpi"
   },
   "source": [
    "#LAB MATERIAL FOR REFERENCE\n",
    "This lab sheet works on MNIST Dataset and discuss about creating Data Loaders, Visualization of a Batch of Training Data, Defining a simple neural network, wandb library, Training the network and other minor toics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "onsxHQmyEWWN"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize the images\n",
    "])\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_file, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        with open(label_file, 'r') as f:\n",
    "            self.labels = f.readlines()\n",
    "        self.img_names = os.listdir(img_dir)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_names[idx])\n",
    "        image = Image.open(img_path)\n",
    "        label = torch.tensor(float(self.labels[idx].strip()) - 1)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = ImageDataset(img_dir = r'C:\\Users\\siddh\\Downloads\\Project Task 1\\train_data',label_file=r'C:\\Users\\siddh\\Downloads\\Project Task 1\\train_labels.txt', transform = transform)\n",
    "train_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "val_data = ImageDataset(img_dir = r'C:\\Users\\siddh\\Downloads\\Project Task 1\\val_data',label_file=r'C:\\Users\\siddh\\Downloads\\Project Task 1\\val_labels.txt', transform = transform)\n",
    "val_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3OkLQFTEWWP"
   },
   "source": [
    "---\n",
    "This cell will create DataLoaders for each of our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdeB1bLfEWWQ"
   },
   "source": [
    "### Visualize a Batch of Training Data\n",
    "\n",
    "The first step in a classification task is to take a look at the data, make sure it is loaded in correctly, then make any initial observations about patterns in that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "xZmLybBlEWWQ",
    "outputId": "51df6d20-fe75-4279-c4bb-631afafa950b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 3, 256, 256), torch.Size([32]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "# images, labels = next(dataiter)\n",
    "images, labels = next(dataiter)\n",
    "images = images.numpy()\n",
    "\n",
    "images.shape,labels.shape\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "# fig = plt.figure(figsize=(25, 4))\n",
    "# for idx in np.arange(20):\n",
    "#     ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "#     img = np.transpose(images[idx], (1, 2, 0))\n",
    "#     #ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "#     ax.imshow(img)\n",
    "#     # print out the correct label for each image\n",
    "#     # .item() gets the value contained in a Tensor\n",
    "#     ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD7FLvbbEWWQ"
   },
   "source": [
    "### View an Image in More Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 984
    },
    "id": "5sVO9v7GEWWQ",
    "outputId": "831883f4-4f1b-4a0b-bf42-01435a744396"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m12\u001b[39m))\n\u001b[0;32m      4\u001b[0m ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m111\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(images[\u001b[43midx\u001b[49m], (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      6\u001b[0m ax\u001b[38;5;241m.\u001b[39mimshow(img, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'idx' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = np.squeeze(images[1])\n",
    "\n",
    "fig = plt.figure(figsize = (12,12))\n",
    "ax = fig.add_subplot(111)\n",
    "img = np.transpose(images[idx], (1, 2, 0))\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 196608)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect all images and labels for PCA fitting\n",
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    all_images.append(images)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "all_images = torch.cat(all_images)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "# all_images.shape, all_labels.shape\n",
    "\n",
    "# Flatten the images\n",
    "all_images_flattened = all_images.view(all_images.size(0), -1).numpy()\n",
    "all_images_flattened.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA on the flattened images\n",
    "pca = PCA(n_components=0.95)  # Adjust n_components as needed\n",
    "pca.fit(all_images_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transform the images using PCA\n",
    "all_images_pca = pca.transform(all_images_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1042)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images_pca.shape\n",
    "# # Create a new dataset with PCA-transformed data\n",
    "# class PCADataset(Dataset):\n",
    "#     def __init__(self, data, labels):\n",
    "#         self.data = data\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image = self.data[idx]\n",
    "#         label = self.labels[idx]\n",
    "#         return torch.tensor(image, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# # Create an instance of the PCA-transformed dataset\n",
    "# pca_dataset = PCADataset(data=all_images_pca, labels=all_labels)\n",
    "\n",
    "# # Create a data loader for the PCA-transformed dataset\n",
    "# pca_loader = DataLoader(dataset=pca_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "# Example: Iterate through the PCA data loader and print the shape of the images and labels\n",
    "# for images, labels in pca_loader:\n",
    "#     print(f'Images batch shape: {images.size()}')\n",
    "#     print(f'Labels batch shape: {labels.size()}')\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvvFgj_-EWWR"
   },
   "source": [
    "---\n",
    "## Define the Network [Architecture](http://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "The architecture will be responsible for seeing as input a 784-dim Tensor of pixel values for each image, and producing a Tensor of length 10 (our number of classes) that indicates the class scores for an input image. This particular example uses two hidden layers and dropout to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xboaGAiIEWWR",
    "outputId": "50b30a19-d08a-48fe-e9cf-c5eb08d59b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=196608, out_features=256, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (batch_norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (batch_norm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=64, out_features=60, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## Define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(all_images_flattened.shape[1],256)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        # Third hidden layer\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc4 = nn.Linear(64, 60)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.batch_norm1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.batch_norm2(self.fc2(x)))\n",
    "        x = self.dropout2(x) \n",
    "        x = F.relu(self.batch_norm3(self.fc3(x)))\n",
    "        x = F.softmax(self.fc4(x), dim=1)  # Softmax for multiclass classification\n",
    "        return x\n",
    "\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOefKx20EWWR"
   },
   "source": [
    "###  Specify [Loss Function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [Optimizer](http://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "It's recommended that you use cross-entropy loss for classification. If you look at the documentation (linked above), you can see that PyTorch's cross entropy function applies a softmax funtion to the output layer *and* then calculates the log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KysPVJk4EWWR"
   },
   "outputs": [],
   "source": [
    "## Specify loss and optimization functions\n",
    "\n",
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCADataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# Create an instance of the PCA-transformed dataset\n",
    "pca_dataset = PCADataset(data=all_images_pca, labels=all_labels)\n",
    "\n",
    "# Create a data loader for the PCA-transformed dataset\n",
    "pca_loader = DataLoader(dataset=pca_dataset, batch_size=32, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, labels) in enumerate(pca_loader):\n",
    "    print(f'Batch {i + 1}')\n",
    "    print(f'Images shape: {images.shape}')\n",
    "    print(f'Labels shape: {labels.shape}')\n",
    "    print(f'Images dtype: {images.dtype}')\n",
    "    print(f'Labels dtype: {labels.dtype}')\n",
    "    print(f'First image: {images[0]}')\n",
    "    print(f'First label: {labels[0]}')\n",
    "    \n",
    "    # Break after inspecting a few batches\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Loss: 4.093173503875732\n",
      "1  Loss: 8.188435077667236\n",
      "2  Loss: 12.281588077545166\n",
      "3  Loss: 16.377042293548584\n",
      "4  Loss: 20.463860511779785\n",
      "5  Loss: 24.553529739379883\n",
      "6  Loss: 28.638224124908447\n",
      "7  Loss: 32.70530414581299\n",
      "8  Loss: 36.79372978210449\n",
      "9  Loss: 40.89035701751709\n",
      "10  Loss: 44.964072704315186\n",
      "11  Loss: 49.054545879364014\n",
      "12  Loss: 53.15126419067383\n",
      "13  Loss: 57.24129676818848\n",
      "14  Loss: 61.32707214355469\n",
      "15  Loss: 65.41596460342407\n",
      "16  Loss: 69.51816082000732\n",
      "17  Loss: 73.61271142959595\n",
      "18  Loss: 77.71410989761353\n",
      "19  Loss: 81.81658267974854\n",
      "20  Loss: 85.91010236740112\n",
      "21  Loss: 89.91829061508179\n",
      "22  Loss: 94.01442861557007\n",
      "23  Loss: 98.09529113769531\n",
      "24  Loss: 102.19591045379639\n",
      "25  Loss: 106.29542541503906\n",
      "26  Loss: 110.39119052886963\n",
      "27  Loss: 114.48547315597534\n",
      "28  Loss: 118.58328199386597\n",
      "29  Loss: 122.6728024482727\n",
      "30  Loss: 126.75844383239746\n",
      "31  Loss: 130.82629346847534\n",
      "32  Loss: 134.88968467712402\n",
      "33  Loss: 138.9710249900818\n",
      "34  Loss: 143.0516152381897\n",
      "35  Loss: 147.08729887008667\n",
      "36  Loss: 151.11554765701294\n",
      "37  Loss: 155.19557666778564\n",
      "38  Loss: 159.2639193534851\n",
      "39  Loss: 163.33664751052856\n",
      "40  Loss: 167.43544387817383\n",
      "41  Loss: 171.4760937690735\n",
      "42  Loss: 175.52871227264404\n",
      "43  Loss: 179.55881595611572\n",
      "44  Loss: 183.62847709655762\n",
      "45  Loss: 187.6962766647339\n",
      "46  Loss: 191.76112937927246\n",
      "47  Loss: 195.83444738388062\n",
      "48  Loss: 199.89776229858398\n",
      "49  Loss: 203.95999145507812\n",
      "50  Loss: 207.9461669921875\n",
      "51  Loss: 212.04198694229126\n",
      "52  Loss: 216.1151843070984\n",
      "53  Loss: 220.11512660980225\n",
      "54  Loss: 224.1877784729004\n",
      "55  Loss: 228.2299942970276\n",
      "56  Loss: 232.2379207611084\n",
      "57  Loss: 236.3268747329712\n",
      "58  Loss: 240.40456533432007\n",
      "59  Loss: 244.45254850387573\n",
      "60  Loss: 248.5172882080078\n",
      "61  Loss: 252.61636924743652\n",
      "62  Loss: 256.71409130096436\n",
      "63  Loss: 260.80693006515503\n",
      "64  Loss: 264.8885555267334\n",
      "65  Loss: 268.96183013916016\n",
      "66  Loss: 273.025839805603\n",
      "67  Loss: 277.0572090148926\n",
      "68  Loss: 281.1636323928833\n",
      "69  Loss: 285.2316746711731\n",
      "70  Loss: 289.25170850753784\n",
      "71  Loss: 293.32513427734375\n",
      "72  Loss: 297.38567304611206\n",
      "73  Loss: 301.4552192687988\n",
      "74  Loss: 305.513201713562\n",
      "75  Loss: 309.59511518478394\n",
      "76  Loss: 313.6978454589844\n",
      "77  Loss: 317.7764525413513\n",
      "78  Loss: 321.7914147377014\n",
      "79  Loss: 325.7890772819519\n",
      "80  Loss: 329.88630962371826\n",
      "81  Loss: 333.99267387390137\n",
      "82  Loss: 338.0346188545227\n",
      "83  Loss: 342.047887802124\n",
      "84  Loss: 346.08021450042725\n",
      "85  Loss: 350.1472444534302\n",
      "86  Loss: 354.1922345161438\n",
      "87  Loss: 358.17477202415466\n",
      "88  Loss: 362.17822003364563\n",
      "89  Loss: 366.20286679267883\n",
      "90  Loss: 370.28424048423767\n",
      "91  Loss: 374.3450939655304\n",
      "92  Loss: 378.35876870155334\n",
      "93  Loss: 382.40028643608093\n",
      "Epoch [1/20], Loss: 4.0681\n",
      "0  Loss: 4.0583014488220215\n",
      "1  Loss: 8.04945993423462\n",
      "2  Loss: 12.09859561920166\n",
      "3  Loss: 16.144343852996826\n",
      "4  Loss: 20.157550811767578\n",
      "5  Loss: 24.166832447052002\n",
      "6  Loss: 28.168115615844727\n",
      "7  Loss: 32.274484634399414\n",
      "8  Loss: 36.36868381500244\n",
      "9  Loss: 40.37045907974243\n",
      "10  Loss: 44.45634365081787\n",
      "11  Loss: 48.514726638793945\n",
      "12  Loss: 52.54583549499512\n",
      "13  Loss: 56.601951122283936\n",
      "14  Loss: 60.7087345123291\n",
      "15  Loss: 64.80290412902832\n",
      "16  Loss: 68.87475395202637\n",
      "17  Loss: 72.92566442489624\n",
      "18  Loss: 77.03791522979736\n",
      "19  Loss: 81.11684703826904\n",
      "20  Loss: 85.15568542480469\n",
      "21  Loss: 89.18798589706421\n",
      "22  Loss: 93.16119694709778\n",
      "23  Loss: 97.17881035804749\n",
      "24  Loss: 101.2060034275055\n",
      "25  Loss: 105.24627327919006\n",
      "26  Loss: 109.29262375831604\n",
      "27  Loss: 113.33636975288391\n",
      "28  Loss: 117.33187437057495\n",
      "29  Loss: 121.30712819099426\n",
      "30  Loss: 125.36220574378967\n",
      "31  Loss: 129.36760067939758\n",
      "32  Loss: 133.46171402931213\n",
      "33  Loss: 137.5134518146515\n",
      "34  Loss: 141.57309794425964\n",
      "35  Loss: 145.555242061615\n",
      "36  Loss: 149.5998511314392\n",
      "37  Loss: 153.55917263031006\n",
      "38  Loss: 157.65664196014404\n",
      "39  Loss: 161.7153124809265\n",
      "40  Loss: 165.78423166275024\n",
      "41  Loss: 169.82728576660156\n",
      "42  Loss: 173.9420804977417\n",
      "43  Loss: 177.91794395446777\n",
      "44  Loss: 181.98151397705078\n",
      "45  Loss: 186.03723335266113\n",
      "46  Loss: 190.03192281723022\n",
      "47  Loss: 194.02998161315918\n",
      "48  Loss: 198.03914546966553\n",
      "49  Loss: 202.07334566116333\n",
      "50  Loss: 206.14038038253784\n",
      "51  Loss: 210.18697023391724\n",
      "52  Loss: 214.24260663986206\n",
      "53  Loss: 218.29662466049194\n",
      "54  Loss: 222.23459649085999\n",
      "55  Loss: 226.30956768989563\n",
      "56  Loss: 230.29840302467346\n",
      "57  Loss: 234.3846137523651\n",
      "58  Loss: 238.34943914413452\n",
      "59  Loss: 242.32035875320435\n",
      "60  Loss: 246.35161638259888\n",
      "61  Loss: 250.2722442150116\n",
      "62  Loss: 254.1651873588562\n",
      "63  Loss: 258.1983003616333\n",
      "64  Loss: 262.1723928451538\n",
      "65  Loss: 266.26334953308105\n",
      "66  Loss: 270.27568435668945\n",
      "67  Loss: 274.2720596790314\n",
      "68  Loss: 278.355633020401\n",
      "69  Loss: 282.4357807636261\n",
      "70  Loss: 286.44935154914856\n",
      "71  Loss: 290.3929281234741\n",
      "72  Loss: 294.41987657546997\n",
      "73  Loss: 298.4103169441223\n",
      "74  Loss: 302.41376972198486\n",
      "75  Loss: 306.4718861579895\n",
      "76  Loss: 310.5245542526245\n",
      "77  Loss: 314.5748200416565\n",
      "78  Loss: 318.57201385498047\n",
      "79  Loss: 322.53453731536865\n",
      "80  Loss: 326.54394006729126\n",
      "81  Loss: 330.6059060096741\n",
      "82  Loss: 334.6294107437134\n",
      "83  Loss: 338.68038606643677\n",
      "84  Loss: 342.66620898246765\n",
      "85  Loss: 346.71993041038513\n",
      "86  Loss: 350.7940671443939\n",
      "87  Loss: 354.84220910072327\n",
      "88  Loss: 358.8483421802521\n",
      "89  Loss: 362.926066160202\n",
      "90  Loss: 366.9920346736908\n",
      "91  Loss: 371.00470423698425\n",
      "92  Loss: 375.1140921115875\n",
      "93  Loss: 379.149551153183\n",
      "Epoch [2/20], Loss: 4.0335\n",
      "0  Loss: 4.082683086395264\n",
      "1  Loss: 8.118732929229736\n",
      "2  Loss: 12.099671840667725\n",
      "3  Loss: 16.090455055236816\n",
      "4  Loss: 20.17327308654785\n",
      "5  Loss: 24.21747064590454\n",
      "6  Loss: 28.2094464302063\n",
      "7  Loss: 32.171008348464966\n",
      "8  Loss: 36.18320822715759\n",
      "9  Loss: 40.160966873168945\n",
      "10  Loss: 44.164785385131836\n",
      "11  Loss: 48.19987726211548\n",
      "12  Loss: 52.24247932434082\n",
      "13  Loss: 56.19770956039429\n",
      "14  Loss: 60.23869848251343\n",
      "15  Loss: 64.27029800415039\n",
      "16  Loss: 68.28828763961792\n",
      "17  Loss: 72.30987215042114\n",
      "18  Loss: 76.29475140571594\n",
      "19  Loss: 80.28988599777222\n",
      "20  Loss: 84.23293042182922\n",
      "21  Loss: 88.25742602348328\n",
      "22  Loss: 92.28118205070496\n",
      "23  Loss: 96.3125627040863\n",
      "24  Loss: 100.39702486991882\n",
      "25  Loss: 104.46469712257385\n",
      "26  Loss: 108.47769951820374\n",
      "27  Loss: 112.44622659683228\n",
      "28  Loss: 116.39752864837646\n",
      "29  Loss: 120.33399271965027\n",
      "30  Loss: 124.378741979599\n",
      "31  Loss: 128.44133639335632\n",
      "32  Loss: 132.46819710731506\n",
      "33  Loss: 136.48768734931946\n",
      "34  Loss: 140.5000660419464\n",
      "35  Loss: 144.55849051475525\n",
      "36  Loss: 148.48333835601807\n",
      "37  Loss: 152.51974534988403\n",
      "38  Loss: 156.57563161849976\n",
      "39  Loss: 160.5469090938568\n",
      "40  Loss: 164.58001589775085\n",
      "41  Loss: 168.6778781414032\n",
      "42  Loss: 172.67327523231506\n",
      "43  Loss: 176.7250120639801\n",
      "44  Loss: 180.729562997818\n",
      "45  Loss: 184.72323489189148\n",
      "46  Loss: 188.69751596450806\n",
      "47  Loss: 192.72176599502563\n",
      "48  Loss: 196.80817127227783\n",
      "49  Loss: 200.85321712493896\n",
      "50  Loss: 204.87350273132324\n",
      "51  Loss: 208.89221477508545\n",
      "52  Loss: 212.96624326705933\n",
      "53  Loss: 216.98532676696777\n",
      "54  Loss: 220.98327445983887\n",
      "55  Loss: 224.96431064605713\n",
      "56  Loss: 228.91263103485107\n",
      "57  Loss: 232.90419960021973\n",
      "58  Loss: 236.95944356918335\n",
      "59  Loss: 240.89567923545837\n",
      "60  Loss: 244.95847249031067\n",
      "61  Loss: 248.98544049263\n",
      "62  Loss: 252.94648218154907\n",
      "63  Loss: 256.83731603622437\n",
      "64  Loss: 260.88145875930786\n",
      "65  Loss: 264.9147992134094\n",
      "66  Loss: 268.9139666557312\n",
      "67  Loss: 272.9737391471863\n",
      "68  Loss: 276.9404227733612\n",
      "69  Loss: 280.97617506980896\n",
      "70  Loss: 284.99075150489807\n",
      "71  Loss: 289.03754925727844\n",
      "72  Loss: 293.06435084342957\n",
      "73  Loss: 297.0827548503876\n",
      "74  Loss: 301.1591532230377\n",
      "75  Loss: 305.1473479270935\n",
      "76  Loss: 309.05572152137756\n",
      "77  Loss: 313.10770630836487\n",
      "78  Loss: 317.17488837242126\n",
      "79  Loss: 321.2616798877716\n",
      "80  Loss: 325.2872188091278\n",
      "81  Loss: 329.2733733654022\n",
      "82  Loss: 333.28208899497986\n",
      "83  Loss: 337.2923948764801\n",
      "84  Loss: 341.34444785118103\n",
      "85  Loss: 345.28033661842346\n",
      "86  Loss: 349.3398163318634\n",
      "87  Loss: 353.2679555416107\n",
      "88  Loss: 357.3089382648468\n",
      "89  Loss: 361.32023072242737\n",
      "90  Loss: 365.31655955314636\n",
      "91  Loss: 369.42585921287537\n",
      "92  Loss: 373.4274809360504\n",
      "93  Loss: 377.5462439060211\n",
      "Epoch [3/20], Loss: 4.0164\n",
      "0  Loss: 3.951936721801758\n",
      "1  Loss: 7.865330934524536\n",
      "2  Loss: 11.841565370559692\n",
      "3  Loss: 15.861657857894897\n",
      "4  Loss: 19.929173231124878\n",
      "5  Loss: 23.907168865203857\n",
      "6  Loss: 27.912994384765625\n",
      "7  Loss: 31.91378355026245\n",
      "8  Loss: 35.8962504863739\n",
      "9  Loss: 39.97741436958313\n",
      "10  Loss: 43.95282196998596\n",
      "11  Loss: 47.99365305900574\n",
      "12  Loss: 51.994532346725464\n",
      "13  Loss: 56.01970839500427\n",
      "14  Loss: 60.13389277458191\n",
      "15  Loss: 64.19273400306702\n",
      "16  Loss: 68.14242124557495\n",
      "17  Loss: 72.07045531272888\n",
      "18  Loss: 76.09659457206726\n",
      "19  Loss: 80.08248281478882\n",
      "20  Loss: 84.07991003990173\n",
      "21  Loss: 88.10100483894348\n",
      "22  Loss: 92.07046055793762\n",
      "23  Loss: 96.11167311668396\n",
      "24  Loss: 100.14948153495789\n",
      "25  Loss: 104.1589949131012\n",
      "26  Loss: 108.10449957847595\n",
      "27  Loss: 112.09540581703186\n",
      "28  Loss: 116.14565587043762\n",
      "29  Loss: 120.1081120967865\n",
      "30  Loss: 124.11996817588806\n",
      "31  Loss: 128.09663915634155\n",
      "32  Loss: 132.20419836044312\n",
      "33  Loss: 136.15117859840393\n",
      "34  Loss: 140.1476447582245\n",
      "35  Loss: 144.23561453819275\n",
      "36  Loss: 148.23520302772522\n",
      "37  Loss: 152.3025143146515\n",
      "38  Loss: 156.41200804710388\n",
      "39  Loss: 160.4483458995819\n",
      "40  Loss: 164.4818193912506\n",
      "41  Loss: 168.49431157112122\n",
      "42  Loss: 172.41284346580505\n",
      "43  Loss: 176.456312417984\n",
      "44  Loss: 180.52552580833435\n",
      "45  Loss: 184.46323823928833\n",
      "46  Loss: 188.46813201904297\n",
      "47  Loss: 192.4883508682251\n",
      "48  Loss: 196.44717931747437\n",
      "49  Loss: 200.50949668884277\n",
      "50  Loss: 204.54151678085327\n",
      "51  Loss: 208.6517481803894\n",
      "52  Loss: 212.5577027797699\n",
      "53  Loss: 216.58867239952087\n",
      "54  Loss: 220.49335074424744\n",
      "55  Loss: 224.48964405059814\n",
      "56  Loss: 228.5421462059021\n",
      "57  Loss: 232.47263932228088\n",
      "58  Loss: 236.4268982410431\n",
      "59  Loss: 240.42563223838806\n",
      "60  Loss: 244.37244939804077\n",
      "61  Loss: 248.42451429367065\n",
      "62  Loss: 252.46337985992432\n",
      "63  Loss: 256.45915603637695\n",
      "64  Loss: 260.5095181465149\n",
      "65  Loss: 264.5194706916809\n",
      "66  Loss: 268.3828945159912\n",
      "67  Loss: 272.329252243042\n",
      "68  Loss: 276.3490948677063\n",
      "69  Loss: 280.3959107398987\n",
      "70  Loss: 284.43279218673706\n",
      "71  Loss: 288.4543881416321\n",
      "72  Loss: 292.4674377441406\n",
      "73  Loss: 296.4954662322998\n",
      "74  Loss: 300.5237412452698\n",
      "75  Loss: 304.52617835998535\n",
      "76  Loss: 308.56847763061523\n",
      "77  Loss: 312.64858055114746\n",
      "78  Loss: 316.67193698883057\n",
      "79  Loss: 320.6165430545807\n",
      "80  Loss: 324.60435819625854\n",
      "81  Loss: 328.56646180152893\n",
      "82  Loss: 332.5560739040375\n",
      "83  Loss: 336.6403658390045\n",
      "84  Loss: 340.6018707752228\n",
      "85  Loss: 344.6436378955841\n",
      "86  Loss: 348.5495924949646\n",
      "87  Loss: 352.6505193710327\n",
      "88  Loss: 356.6324896812439\n",
      "89  Loss: 360.67704582214355\n",
      "90  Loss: 364.668710231781\n",
      "91  Loss: 368.70497274398804\n",
      "92  Loss: 372.7538433074951\n",
      "93  Loss: 376.83753967285156\n",
      "Epoch [4/20], Loss: 4.0089\n",
      "0  Loss: 4.051181793212891\n",
      "1  Loss: 7.983826398849487\n",
      "2  Loss: 12.005638360977173\n",
      "3  Loss: 16.027832746505737\n",
      "4  Loss: 20.115649461746216\n",
      "5  Loss: 24.151164770126343\n",
      "6  Loss: 28.113101959228516\n",
      "7  Loss: 32.07091212272644\n",
      "8  Loss: 36.0718457698822\n",
      "9  Loss: 40.093884229660034\n",
      "10  Loss: 44.09379196166992\n",
      "11  Loss: 47.98284387588501\n",
      "12  Loss: 51.87783360481262\n",
      "13  Loss: 55.856449842453\n",
      "14  Loss: 59.937777280807495\n",
      "15  Loss: 63.99543118476868\n",
      "16  Loss: 67.94719552993774\n",
      "17  Loss: 71.86314272880554\n",
      "18  Loss: 75.86159181594849\n",
      "19  Loss: 79.89956665039062\n",
      "20  Loss: 83.91169500350952\n",
      "21  Loss: 87.93560600280762\n",
      "22  Loss: 91.94281578063965\n",
      "23  Loss: 95.9332070350647\n",
      "24  Loss: 99.915700674057\n",
      "25  Loss: 103.97125029563904\n",
      "26  Loss: 108.02647709846497\n",
      "27  Loss: 112.06190276145935\n",
      "28  Loss: 116.05437755584717\n",
      "29  Loss: 120.08658933639526\n",
      "30  Loss: 124.10928201675415\n",
      "31  Loss: 128.1020016670227\n",
      "32  Loss: 132.1471848487854\n",
      "33  Loss: 136.18440198898315\n",
      "34  Loss: 140.25971698760986\n",
      "35  Loss: 144.20189762115479\n",
      "36  Loss: 148.22477340698242\n",
      "37  Loss: 152.18476629257202\n",
      "38  Loss: 156.19288873672485\n",
      "39  Loss: 160.25532007217407\n",
      "40  Loss: 164.27212524414062\n",
      "41  Loss: 168.2428638935089\n",
      "42  Loss: 172.32281517982483\n",
      "43  Loss: 176.37394261360168\n",
      "44  Loss: 180.3005828857422\n",
      "45  Loss: 184.34451055526733\n",
      "46  Loss: 188.26172614097595\n",
      "47  Loss: 192.267564535141\n",
      "48  Loss: 196.17205119132996\n",
      "49  Loss: 200.12309193611145\n",
      "50  Loss: 204.048743724823\n",
      "51  Loss: 207.9956192970276\n",
      "52  Loss: 211.92025327682495\n",
      "53  Loss: 215.91403579711914\n",
      "54  Loss: 219.91765022277832\n",
      "55  Loss: 223.93223810195923\n",
      "56  Loss: 227.97516918182373\n",
      "57  Loss: 231.9679775238037\n",
      "58  Loss: 235.91025829315186\n",
      "59  Loss: 239.86157941818237\n",
      "60  Loss: 243.85947704315186\n",
      "61  Loss: 247.76904225349426\n",
      "62  Loss: 251.78713011741638\n",
      "63  Loss: 255.7868914604187\n",
      "64  Loss: 259.72254276275635\n",
      "65  Loss: 263.69276189804077\n",
      "66  Loss: 267.70000886917114\n",
      "67  Loss: 271.63741970062256\n",
      "68  Loss: 275.6575336456299\n",
      "69  Loss: 279.48459649086\n",
      "70  Loss: 283.55553221702576\n",
      "71  Loss: 287.5030961036682\n",
      "72  Loss: 291.5362515449524\n",
      "73  Loss: 295.5215394496918\n",
      "74  Loss: 299.5313594341278\n",
      "75  Loss: 303.53938603401184\n",
      "76  Loss: 307.48294043540955\n",
      "77  Loss: 311.4842994213104\n",
      "78  Loss: 315.4223048686981\n",
      "79  Loss: 319.3856461048126\n",
      "80  Loss: 323.36464619636536\n",
      "81  Loss: 327.4229967594147\n",
      "82  Loss: 331.4524872303009\n",
      "83  Loss: 335.3381931781769\n",
      "84  Loss: 339.358815908432\n",
      "85  Loss: 343.27702164649963\n",
      "86  Loss: 347.2129545211792\n",
      "87  Loss: 351.2089412212372\n",
      "88  Loss: 355.18260073661804\n",
      "89  Loss: 359.2344195842743\n",
      "90  Loss: 363.32332015037537\n",
      "91  Loss: 367.3001227378845\n",
      "92  Loss: 371.2360301017761\n",
      "93  Loss: 375.2676658630371\n",
      "Epoch [5/20], Loss: 3.9922\n",
      "0  Loss: 3.9043588638305664\n",
      "1  Loss: 7.873439311981201\n",
      "2  Loss: 11.893782138824463\n",
      "3  Loss: 15.859609603881836\n",
      "4  Loss: 19.824172019958496\n",
      "5  Loss: 23.802873134613037\n",
      "6  Loss: 27.82051658630371\n",
      "7  Loss: 31.939191818237305\n",
      "8  Loss: 35.87480330467224\n",
      "9  Loss: 39.834439754486084\n",
      "10  Loss: 43.7787606716156\n",
      "11  Loss: 47.77495312690735\n",
      "12  Loss: 51.7204487323761\n",
      "13  Loss: 55.72562050819397\n",
      "14  Loss: 59.66917538642883\n",
      "15  Loss: 63.66306781768799\n",
      "16  Loss: 67.6443042755127\n",
      "17  Loss: 71.61536073684692\n",
      "18  Loss: 75.59626746177673\n",
      "19  Loss: 79.58671545982361\n",
      "20  Loss: 83.58600950241089\n",
      "21  Loss: 87.5960841178894\n",
      "22  Loss: 91.55607414245605\n",
      "23  Loss: 95.52156925201416\n",
      "24  Loss: 99.45485258102417\n",
      "25  Loss: 103.4993314743042\n",
      "26  Loss: 107.49878311157227\n",
      "27  Loss: 111.48312783241272\n",
      "28  Loss: 115.43416786193848\n",
      "29  Loss: 119.39576315879822\n",
      "30  Loss: 123.42860770225525\n",
      "31  Loss: 127.34879183769226\n",
      "32  Loss: 131.30325174331665\n",
      "33  Loss: 135.21962118148804\n",
      "34  Loss: 139.1851668357849\n",
      "35  Loss: 143.17173099517822\n",
      "36  Loss: 147.1929087638855\n",
      "37  Loss: 151.10085201263428\n",
      "38  Loss: 155.0896384716034\n",
      "39  Loss: 159.09725832939148\n",
      "40  Loss: 163.13371348381042\n",
      "41  Loss: 167.15484833717346\n",
      "42  Loss: 171.1737678050995\n",
      "43  Loss: 175.16336369514465\n",
      "44  Loss: 179.25438952445984\n",
      "45  Loss: 183.30381560325623\n",
      "46  Loss: 187.37294268608093\n",
      "47  Loss: 191.32263946533203\n",
      "48  Loss: 195.32301950454712\n",
      "49  Loss: 199.31038975715637\n",
      "50  Loss: 203.25813817977905\n",
      "51  Loss: 207.23208141326904\n",
      "52  Loss: 211.28493642807007\n",
      "53  Loss: 215.3758420944214\n",
      "54  Loss: 219.36445140838623\n",
      "55  Loss: 223.28490567207336\n",
      "56  Loss: 227.29364705085754\n",
      "57  Loss: 231.29169631004333\n",
      "58  Loss: 235.2948272228241\n",
      "59  Loss: 239.38619017601013\n",
      "60  Loss: 243.26858568191528\n",
      "61  Loss: 247.25911235809326\n",
      "62  Loss: 251.2216875553131\n",
      "63  Loss: 255.2583200931549\n",
      "64  Loss: 259.2596185207367\n",
      "65  Loss: 263.26442074775696\n",
      "66  Loss: 267.2878067493439\n",
      "67  Loss: 271.27897691726685\n",
      "68  Loss: 275.3009834289551\n",
      "69  Loss: 279.40160846710205\n",
      "70  Loss: 283.29377365112305\n",
      "71  Loss: 287.28205132484436\n",
      "72  Loss: 291.34675765037537\n",
      "73  Loss: 295.37846207618713\n",
      "74  Loss: 299.3768780231476\n",
      "75  Loss: 303.314820766449\n",
      "76  Loss: 307.34345722198486\n",
      "77  Loss: 311.181259393692\n",
      "78  Loss: 315.1439845561981\n",
      "79  Loss: 319.2480857372284\n",
      "80  Loss: 323.2088668346405\n",
      "81  Loss: 327.15090465545654\n",
      "82  Loss: 331.13100123405457\n",
      "83  Loss: 335.17864537239075\n",
      "84  Loss: 339.2049539089203\n",
      "85  Loss: 343.2620213031769\n",
      "86  Loss: 347.20279002189636\n",
      "87  Loss: 351.14734268188477\n",
      "88  Loss: 355.1600532531738\n",
      "89  Loss: 359.1044445037842\n",
      "90  Loss: 363.07422494888306\n",
      "91  Loss: 367.00977063179016\n",
      "92  Loss: 371.0551760196686\n",
      "93  Loss: 375.111127614975\n",
      "Epoch [6/20], Loss: 3.9905\n",
      "0  Loss: 4.025003433227539\n",
      "1  Loss: 8.072709083557129\n",
      "2  Loss: 12.046228647232056\n",
      "3  Loss: 16.01024055480957\n",
      "4  Loss: 19.986258506774902\n",
      "5  Loss: 23.944617748260498\n",
      "6  Loss: 27.961315631866455\n",
      "7  Loss: 31.85228395462036\n",
      "8  Loss: 35.81806802749634\n",
      "9  Loss: 39.68733310699463\n",
      "10  Loss: 43.70426607131958\n",
      "11  Loss: 47.68325757980347\n",
      "12  Loss: 51.61168122291565\n",
      "13  Loss: 55.69397759437561\n",
      "14  Loss: 59.68832969665527\n",
      "15  Loss: 63.698241233825684\n",
      "16  Loss: 67.69891452789307\n",
      "17  Loss: 71.64975476264954\n",
      "18  Loss: 75.58399033546448\n",
      "19  Loss: 79.59049582481384\n",
      "20  Loss: 83.53320264816284\n",
      "21  Loss: 87.48110938072205\n",
      "22  Loss: 91.41380858421326\n",
      "23  Loss: 95.37269759178162\n",
      "24  Loss: 99.39025568962097\n",
      "25  Loss: 103.35827040672302\n",
      "26  Loss: 107.28637981414795\n",
      "27  Loss: 111.25470089912415\n",
      "28  Loss: 115.31028628349304\n",
      "29  Loss: 119.3005850315094\n",
      "30  Loss: 123.27895522117615\n",
      "31  Loss: 127.22938084602356\n",
      "32  Loss: 131.20605492591858\n",
      "33  Loss: 135.22454047203064\n",
      "34  Loss: 139.26903414726257\n",
      "35  Loss: 143.2806613445282\n",
      "36  Loss: 147.20732235908508\n",
      "37  Loss: 151.22936844825745\n",
      "38  Loss: 155.22191047668457\n",
      "39  Loss: 159.25774002075195\n",
      "40  Loss: 163.31339263916016\n",
      "41  Loss: 167.27141880989075\n",
      "42  Loss: 171.1968274116516\n",
      "43  Loss: 175.16030621528625\n",
      "44  Loss: 179.02299308776855\n",
      "45  Loss: 183.03531455993652\n",
      "46  Loss: 187.0299208164215\n",
      "47  Loss: 190.87393140792847\n",
      "48  Loss: 194.85793256759644\n",
      "49  Loss: 198.78043961524963\n",
      "50  Loss: 202.8993170261383\n",
      "51  Loss: 206.86681747436523\n",
      "52  Loss: 210.83953857421875\n",
      "53  Loss: 214.82196855545044\n",
      "54  Loss: 218.8237428665161\n",
      "55  Loss: 222.7261824607849\n",
      "56  Loss: 226.64974403381348\n",
      "57  Loss: 230.6788969039917\n",
      "58  Loss: 234.61748099327087\n",
      "59  Loss: 238.56507325172424\n",
      "60  Loss: 242.52192950248718\n",
      "61  Loss: 246.519122838974\n",
      "62  Loss: 250.5442521572113\n",
      "63  Loss: 254.56774401664734\n",
      "64  Loss: 258.5887830257416\n",
      "65  Loss: 262.5998661518097\n",
      "66  Loss: 266.62434935569763\n",
      "67  Loss: 270.6572344303131\n",
      "68  Loss: 274.5711488723755\n",
      "69  Loss: 278.5655550956726\n",
      "70  Loss: 282.5480329990387\n",
      "71  Loss: 286.5057520866394\n",
      "72  Loss: 290.364004611969\n",
      "73  Loss: 294.3232808113098\n",
      "74  Loss: 298.2020962238312\n",
      "75  Loss: 302.16579031944275\n",
      "76  Loss: 306.1155319213867\n",
      "77  Loss: 310.0109541416168\n",
      "78  Loss: 313.9493715763092\n",
      "79  Loss: 317.8584187030792\n",
      "80  Loss: 321.85249423980713\n",
      "81  Loss: 325.7845652103424\n",
      "82  Loss: 329.7603449821472\n",
      "83  Loss: 333.7565462589264\n",
      "84  Loss: 337.75005412101746\n",
      "85  Loss: 341.7881610393524\n",
      "86  Loss: 345.7521994113922\n",
      "87  Loss: 349.6799383163452\n",
      "88  Loss: 353.62756729125977\n",
      "89  Loss: 357.6020600795746\n",
      "90  Loss: 361.4910092353821\n",
      "91  Loss: 365.41371297836304\n",
      "92  Loss: 369.41187357902527\n",
      "93  Loss: 373.3532495498657\n",
      "Epoch [7/20], Loss: 3.9718\n",
      "0  Loss: 3.96945858001709\n",
      "1  Loss: 7.946029424667358\n",
      "2  Loss: 11.982099294662476\n",
      "3  Loss: 15.913353681564331\n",
      "4  Loss: 19.878400564193726\n",
      "5  Loss: 23.813554763793945\n",
      "6  Loss: 27.796387195587158\n",
      "7  Loss: 31.71491050720215\n",
      "8  Loss: 35.67472052574158\n",
      "9  Loss: 39.575395822525024\n",
      "10  Loss: 43.45691251754761\n",
      "11  Loss: 47.42375874519348\n",
      "12  Loss: 51.413604736328125\n",
      "13  Loss: 55.382832527160645\n",
      "14  Loss: 59.42955446243286\n",
      "15  Loss: 63.3567955493927\n",
      "16  Loss: 67.35032153129578\n",
      "17  Loss: 71.28671979904175\n",
      "18  Loss: 75.35648107528687\n",
      "19  Loss: 79.24893522262573\n",
      "20  Loss: 83.20637083053589\n",
      "21  Loss: 87.14970135688782\n",
      "22  Loss: 91.09684777259827\n",
      "23  Loss: 95.1651360988617\n",
      "24  Loss: 99.24437022209167\n",
      "25  Loss: 103.22702813148499\n",
      "26  Loss: 107.171217918396\n",
      "27  Loss: 111.27524662017822\n",
      "28  Loss: 115.26035666465759\n",
      "29  Loss: 119.29527449607849\n",
      "30  Loss: 123.28475785255432\n",
      "31  Loss: 127.26677298545837\n",
      "32  Loss: 131.224360704422\n",
      "33  Loss: 135.20132327079773\n",
      "34  Loss: 139.11146020889282\n",
      "35  Loss: 143.13940572738647\n",
      "36  Loss: 147.06367373466492\n",
      "37  Loss: 151.09735751152039\n",
      "38  Loss: 155.0318787097931\n",
      "39  Loss: 159.06702971458435\n",
      "40  Loss: 162.99398350715637\n",
      "41  Loss: 167.07982277870178\n",
      "42  Loss: 171.12043833732605\n",
      "43  Loss: 175.08174586296082\n",
      "44  Loss: 179.0031337738037\n",
      "45  Loss: 183.03185033798218\n",
      "46  Loss: 187.00399327278137\n",
      "47  Loss: 191.02909398078918\n",
      "48  Loss: 194.99093341827393\n",
      "49  Loss: 198.97293496131897\n",
      "50  Loss: 202.87966656684875\n",
      "51  Loss: 206.85969853401184\n",
      "52  Loss: 210.86777234077454\n",
      "53  Loss: 214.71704721450806\n",
      "54  Loss: 218.66896271705627\n",
      "55  Loss: 222.67377066612244\n",
      "56  Loss: 226.7482521533966\n",
      "57  Loss: 230.79887795448303\n",
      "58  Loss: 234.81794810295105\n",
      "59  Loss: 238.82713198661804\n",
      "60  Loss: 242.79330706596375\n",
      "61  Loss: 246.77919912338257\n",
      "62  Loss: 250.81243991851807\n",
      "63  Loss: 254.80669379234314\n",
      "64  Loss: 258.83615374565125\n",
      "65  Loss: 262.8036079406738\n",
      "66  Loss: 266.8910984992981\n",
      "67  Loss: 270.81363224983215\n",
      "68  Loss: 274.7496967315674\n",
      "69  Loss: 278.67173886299133\n",
      "70  Loss: 282.72939896583557\n",
      "71  Loss: 286.73209595680237\n",
      "72  Loss: 290.6966986656189\n",
      "73  Loss: 294.68452191352844\n",
      "74  Loss: 298.6712474822998\n",
      "75  Loss: 302.57645630836487\n",
      "76  Loss: 306.6415240764618\n",
      "77  Loss: 310.46013855934143\n",
      "78  Loss: 314.4704210758209\n",
      "79  Loss: 318.42408633232117\n",
      "80  Loss: 322.29895973205566\n",
      "81  Loss: 326.2572226524353\n",
      "82  Loss: 330.17517614364624\n",
      "83  Loss: 334.1132695674896\n",
      "84  Loss: 338.0768506526947\n",
      "85  Loss: 342.0215587615967\n",
      "86  Loss: 345.98805809020996\n",
      "87  Loss: 349.9572083950043\n",
      "88  Loss: 353.9431064128876\n",
      "89  Loss: 357.9794747829437\n",
      "90  Loss: 361.96098923683167\n",
      "91  Loss: 365.8456721305847\n",
      "92  Loss: 369.89567375183105\n",
      "93  Loss: 373.92911767959595\n",
      "Epoch [8/20], Loss: 3.9780\n",
      "0  Loss: 4.041346073150635\n",
      "1  Loss: 8.019269466400146\n",
      "2  Loss: 11.8344144821167\n",
      "3  Loss: 15.701918840408325\n",
      "4  Loss: 19.593614101409912\n",
      "5  Loss: 23.51962947845459\n",
      "6  Loss: 27.45818853378296\n",
      "7  Loss: 31.51168966293335\n",
      "8  Loss: 35.41359758377075\n",
      "9  Loss: 39.32374405860901\n",
      "10  Loss: 43.32647395133972\n",
      "11  Loss: 47.33470892906189\n",
      "12  Loss: 51.20499396324158\n",
      "13  Loss: 55.30815863609314\n",
      "14  Loss: 59.25895094871521\n",
      "15  Loss: 63.28533434867859\n",
      "16  Loss: 67.30471444129944\n",
      "17  Loss: 71.2712037563324\n",
      "18  Loss: 75.23496794700623\n",
      "19  Loss: 79.20774912834167\n",
      "20  Loss: 83.1790120601654\n",
      "21  Loss: 87.14633393287659\n",
      "22  Loss: 91.09991717338562\n",
      "23  Loss: 95.05288815498352\n",
      "24  Loss: 98.98765397071838\n",
      "25  Loss: 102.9738712310791\n",
      "26  Loss: 107.0113525390625\n",
      "27  Loss: 111.01633310317993\n",
      "28  Loss: 114.94325947761536\n",
      "29  Loss: 118.9152581691742\n",
      "30  Loss: 122.75944828987122\n",
      "31  Loss: 126.64132142066956\n",
      "32  Loss: 130.57533955574036\n",
      "33  Loss: 134.58145928382874\n",
      "34  Loss: 138.5517327785492\n",
      "35  Loss: 142.46454238891602\n",
      "36  Loss: 146.40304803848267\n",
      "37  Loss: 150.37840580940247\n",
      "38  Loss: 154.2917833328247\n",
      "39  Loss: 158.302592754364\n",
      "40  Loss: 162.2845423221588\n",
      "41  Loss: 166.1304030418396\n",
      "42  Loss: 170.12573075294495\n",
      "43  Loss: 174.09426259994507\n",
      "44  Loss: 178.0751075744629\n",
      "45  Loss: 181.95201683044434\n",
      "46  Loss: 185.9045672416687\n",
      "47  Loss: 189.80797171592712\n",
      "48  Loss: 193.7711787223816\n",
      "49  Loss: 197.7517590522766\n",
      "50  Loss: 201.7655758857727\n",
      "51  Loss: 205.77171564102173\n",
      "52  Loss: 209.89074993133545\n",
      "53  Loss: 213.8627347946167\n",
      "54  Loss: 217.77999639511108\n",
      "55  Loss: 221.6757972240448\n",
      "56  Loss: 225.62807369232178\n",
      "57  Loss: 229.67713022232056\n",
      "58  Loss: 233.68463850021362\n",
      "59  Loss: 237.6377420425415\n",
      "60  Loss: 241.53626990318298\n",
      "61  Loss: 245.46379804611206\n",
      "62  Loss: 249.42822527885437\n",
      "63  Loss: 253.40996289253235\n",
      "64  Loss: 257.3852689266205\n",
      "65  Loss: 261.28628492355347\n",
      "66  Loss: 265.2197983264923\n",
      "67  Loss: 269.12982201576233\n",
      "68  Loss: 273.1666810512543\n",
      "69  Loss: 277.10093331336975\n",
      "70  Loss: 281.0378768444061\n",
      "71  Loss: 285.09499287605286\n",
      "72  Loss: 289.0414984226227\n",
      "73  Loss: 293.0033197402954\n",
      "74  Loss: 297.00504207611084\n",
      "75  Loss: 300.8917715549469\n",
      "76  Loss: 304.80803084373474\n",
      "77  Loss: 308.7994194030762\n",
      "78  Loss: 312.7619469165802\n",
      "79  Loss: 316.8336732387543\n",
      "80  Loss: 320.8637592792511\n",
      "81  Loss: 324.78016686439514\n",
      "82  Loss: 328.7094819545746\n",
      "83  Loss: 332.57843494415283\n",
      "84  Loss: 336.5271530151367\n",
      "85  Loss: 340.58560609817505\n",
      "86  Loss: 344.4988372325897\n",
      "87  Loss: 348.43942618370056\n",
      "88  Loss: 352.32540583610535\n",
      "89  Loss: 356.30800580978394\n",
      "90  Loss: 360.31592321395874\n",
      "91  Loss: 364.28897619247437\n",
      "92  Loss: 368.3223919868469\n",
      "93  Loss: 372.26187777519226\n",
      "Epoch [9/20], Loss: 3.9602\n",
      "0  Loss: 4.017735481262207\n",
      "1  Loss: 8.006056070327759\n",
      "2  Loss: 11.900744199752808\n",
      "3  Loss: 15.83422589302063\n",
      "4  Loss: 19.72552800178528\n",
      "5  Loss: 23.6429762840271\n",
      "6  Loss: 27.57092547416687\n",
      "7  Loss: 31.476617336273193\n",
      "8  Loss: 35.443352937698364\n",
      "9  Loss: 39.413713693618774\n",
      "10  Loss: 43.451839208602905\n",
      "11  Loss: 47.417320251464844\n",
      "12  Loss: 51.41904401779175\n",
      "13  Loss: 55.36788010597229\n",
      "14  Loss: 59.36329698562622\n",
      "15  Loss: 63.29187226295471\n",
      "16  Loss: 67.24118256568909\n",
      "17  Loss: 71.2943160533905\n",
      "18  Loss: 75.37083601951599\n",
      "19  Loss: 79.31485271453857\n",
      "20  Loss: 83.34634160995483\n",
      "21  Loss: 87.33862328529358\n",
      "22  Loss: 91.1855640411377\n",
      "23  Loss: 95.20226573944092\n",
      "24  Loss: 99.206307888031\n",
      "25  Loss: 103.11078143119812\n",
      "26  Loss: 107.10717964172363\n",
      "27  Loss: 111.04275965690613\n",
      "28  Loss: 115.07836508750916\n",
      "29  Loss: 119.11387133598328\n",
      "30  Loss: 122.99273943901062\n",
      "31  Loss: 126.9764404296875\n",
      "32  Loss: 130.93928289413452\n",
      "33  Loss: 134.93306231498718\n",
      "34  Loss: 138.85999155044556\n",
      "35  Loss: 142.8252866268158\n",
      "36  Loss: 146.79752898216248\n",
      "37  Loss: 150.58918261528015\n",
      "38  Loss: 154.58495998382568\n",
      "39  Loss: 158.5377378463745\n",
      "40  Loss: 162.4666576385498\n",
      "41  Loss: 166.45859909057617\n",
      "42  Loss: 170.46427822113037\n",
      "43  Loss: 174.42740678787231\n",
      "44  Loss: 178.49157667160034\n",
      "45  Loss: 182.4449224472046\n",
      "46  Loss: 186.34981274604797\n",
      "47  Loss: 190.37474465370178\n",
      "48  Loss: 194.33550882339478\n",
      "49  Loss: 198.0947835445404\n",
      "50  Loss: 202.09059405326843\n",
      "51  Loss: 205.99395084381104\n",
      "52  Loss: 210.0138702392578\n",
      "53  Loss: 214.03287267684937\n",
      "54  Loss: 218.13104438781738\n",
      "55  Loss: 222.12192487716675\n",
      "56  Loss: 226.12942457199097\n",
      "57  Loss: 230.0829563140869\n",
      "58  Loss: 234.00040817260742\n",
      "59  Loss: 237.86995792388916\n",
      "60  Loss: 241.84720540046692\n",
      "61  Loss: 245.65696454048157\n",
      "62  Loss: 249.69351077079773\n",
      "63  Loss: 253.53485345840454\n",
      "64  Loss: 257.4357314109802\n",
      "65  Loss: 261.2955119609833\n",
      "66  Loss: 265.28478384017944\n",
      "67  Loss: 269.3211693763733\n",
      "68  Loss: 273.13987946510315\n",
      "69  Loss: 277.1868374347687\n",
      "70  Loss: 281.189653635025\n",
      "71  Loss: 285.09163880348206\n",
      "72  Loss: 289.04647994041443\n",
      "73  Loss: 293.03817558288574\n",
      "74  Loss: 296.9452655315399\n",
      "75  Loss: 300.8274745941162\n",
      "76  Loss: 304.7086808681488\n",
      "77  Loss: 308.67224049568176\n",
      "78  Loss: 312.7279169559479\n",
      "79  Loss: 316.51619052886963\n",
      "80  Loss: 320.49804496765137\n",
      "81  Loss: 324.4409248828888\n",
      "82  Loss: 328.2570343017578\n",
      "83  Loss: 332.20459175109863\n",
      "84  Loss: 336.16752219200134\n",
      "85  Loss: 340.0256419181824\n",
      "86  Loss: 344.0352268218994\n",
      "87  Loss: 347.8996162414551\n",
      "88  Loss: 351.8656690120697\n",
      "89  Loss: 355.91106152534485\n",
      "90  Loss: 359.8643298149109\n",
      "91  Loss: 363.8352041244507\n",
      "92  Loss: 367.71631932258606\n",
      "93  Loss: 371.7248537540436\n",
      "Epoch [10/20], Loss: 3.9545\n",
      "0  Loss: 4.075112342834473\n",
      "1  Loss: 7.977332353591919\n",
      "2  Loss: 11.989325284957886\n",
      "3  Loss: 16.096473932266235\n",
      "4  Loss: 20.09881615638733\n",
      "5  Loss: 23.90936803817749\n",
      "6  Loss: 27.9386944770813\n",
      "7  Loss: 31.83992052078247\n",
      "8  Loss: 35.79070281982422\n",
      "9  Loss: 39.83879804611206\n",
      "10  Loss: 43.80099153518677\n",
      "11  Loss: 47.78620457649231\n",
      "12  Loss: 51.74318718910217\n",
      "13  Loss: 55.7291464805603\n",
      "14  Loss: 59.61230731010437\n",
      "15  Loss: 63.53085446357727\n",
      "16  Loss: 67.44412350654602\n",
      "17  Loss: 71.3574059009552\n",
      "18  Loss: 75.17790651321411\n",
      "19  Loss: 79.14688920974731\n",
      "20  Loss: 82.98149728775024\n",
      "21  Loss: 87.04595279693604\n",
      "22  Loss: 91.0337381362915\n",
      "23  Loss: 95.02355980873108\n",
      "24  Loss: 99.02075672149658\n",
      "25  Loss: 103.03393459320068\n",
      "26  Loss: 106.99151301383972\n",
      "27  Loss: 110.96767854690552\n",
      "28  Loss: 115.00670003890991\n",
      "29  Loss: 118.95659399032593\n",
      "30  Loss: 123.06197595596313\n",
      "31  Loss: 127.11936092376709\n",
      "32  Loss: 131.0382101535797\n",
      "33  Loss: 134.99830222129822\n",
      "34  Loss: 138.97008872032166\n",
      "35  Loss: 142.88272333145142\n",
      "36  Loss: 146.7702260017395\n",
      "37  Loss: 150.73532605171204\n",
      "38  Loss: 154.715580701828\n",
      "39  Loss: 158.77546286582947\n",
      "40  Loss: 162.7513427734375\n",
      "41  Loss: 166.74779438972473\n",
      "42  Loss: 170.75141310691833\n",
      "43  Loss: 174.67765021324158\n",
      "44  Loss: 178.66423559188843\n",
      "45  Loss: 182.63042306900024\n",
      "46  Loss: 186.63676357269287\n",
      "47  Loss: 190.55241751670837\n",
      "48  Loss: 194.5329122543335\n",
      "49  Loss: 198.57155513763428\n",
      "50  Loss: 202.39264917373657\n",
      "51  Loss: 206.3535132408142\n",
      "52  Loss: 210.27677130699158\n",
      "53  Loss: 214.27730250358582\n",
      "54  Loss: 218.15642952919006\n",
      "55  Loss: 222.03746032714844\n",
      "56  Loss: 225.97257328033447\n",
      "57  Loss: 229.93901705741882\n",
      "58  Loss: 233.85619521141052\n",
      "59  Loss: 237.82420754432678\n",
      "60  Loss: 241.8453552722931\n",
      "61  Loss: 245.8256447315216\n",
      "62  Loss: 249.70532274246216\n",
      "63  Loss: 253.64261293411255\n",
      "64  Loss: 257.5896506309509\n",
      "65  Loss: 261.4919707775116\n",
      "66  Loss: 265.47486090660095\n",
      "67  Loss: 269.3661494255066\n",
      "68  Loss: 273.3726325035095\n",
      "69  Loss: 277.43162393569946\n",
      "70  Loss: 281.457896232605\n",
      "71  Loss: 285.3587884902954\n",
      "72  Loss: 289.2881135940552\n",
      "73  Loss: 293.19906067848206\n",
      "74  Loss: 297.2396800518036\n",
      "75  Loss: 301.2552411556244\n",
      "76  Loss: 305.27273392677307\n",
      "77  Loss: 309.3222978115082\n",
      "78  Loss: 313.29147267341614\n",
      "79  Loss: 317.1535816192627\n",
      "80  Loss: 321.205988407135\n",
      "81  Loss: 325.2639055252075\n",
      "82  Loss: 329.3710660934448\n",
      "83  Loss: 333.48749685287476\n",
      "84  Loss: 337.41147446632385\n",
      "85  Loss: 341.2851548194885\n",
      "86  Loss: 345.1781315803528\n",
      "87  Loss: 349.107209444046\n",
      "88  Loss: 353.14848732948303\n",
      "89  Loss: 357.07886385917664\n",
      "90  Loss: 361.0645577907562\n",
      "91  Loss: 365.03801131248474\n",
      "92  Loss: 368.9653785228729\n",
      "93  Loss: 373.03900265693665\n",
      "Epoch [11/20], Loss: 3.9685\n",
      "0  Loss: 3.9516727924346924\n",
      "1  Loss: 7.906324148178101\n",
      "2  Loss: 11.910392999649048\n",
      "3  Loss: 15.908226251602173\n",
      "4  Loss: 19.90449285507202\n",
      "5  Loss: 23.931152820587158\n",
      "6  Loss: 27.817511558532715\n",
      "7  Loss: 31.72919797897339\n",
      "8  Loss: 35.72818946838379\n",
      "9  Loss: 39.65598773956299\n",
      "10  Loss: 43.56687688827515\n",
      "11  Loss: 47.526212215423584\n",
      "12  Loss: 51.52167320251465\n",
      "13  Loss: 55.444063901901245\n",
      "14  Loss: 59.34344172477722\n",
      "15  Loss: 63.30573749542236\n",
      "16  Loss: 67.29957389831543\n",
      "17  Loss: 71.26948618888855\n",
      "18  Loss: 75.25226926803589\n",
      "19  Loss: 79.15784025192261\n",
      "20  Loss: 83.08248209953308\n",
      "21  Loss: 87.03797793388367\n",
      "22  Loss: 91.0722267627716\n",
      "23  Loss: 95.04711961746216\n",
      "24  Loss: 98.96721482276917\n",
      "25  Loss: 102.77867102622986\n",
      "26  Loss: 106.66914987564087\n",
      "27  Loss: 110.66037774085999\n",
      "28  Loss: 114.6300790309906\n",
      "29  Loss: 118.63215231895447\n",
      "30  Loss: 122.58554244041443\n",
      "31  Loss: 126.60502123832703\n",
      "32  Loss: 130.55747961997986\n",
      "33  Loss: 134.54402542114258\n",
      "34  Loss: 138.4498736858368\n",
      "35  Loss: 142.3246352672577\n",
      "36  Loss: 146.24011850357056\n",
      "37  Loss: 150.25227165222168\n",
      "38  Loss: 154.2083113193512\n",
      "39  Loss: 158.13179683685303\n",
      "40  Loss: 161.87009835243225\n",
      "41  Loss: 165.78042817115784\n",
      "42  Loss: 169.72549676895142\n",
      "43  Loss: 173.65677118301392\n",
      "44  Loss: 177.64587020874023\n",
      "45  Loss: 181.65986394882202\n",
      "46  Loss: 185.60261058807373\n",
      "47  Loss: 189.57137989997864\n",
      "48  Loss: 193.51265263557434\n",
      "49  Loss: 197.39682960510254\n",
      "50  Loss: 201.35372638702393\n",
      "51  Loss: 205.35622835159302\n",
      "52  Loss: 209.31075382232666\n",
      "53  Loss: 213.19401478767395\n",
      "54  Loss: 217.15505266189575\n",
      "55  Loss: 221.08360958099365\n",
      "56  Loss: 225.0225214958191\n",
      "57  Loss: 228.98739862442017\n",
      "58  Loss: 233.0756106376648\n",
      "59  Loss: 237.1672706604004\n",
      "60  Loss: 241.16995859146118\n",
      "61  Loss: 245.14162921905518\n",
      "62  Loss: 249.01466012001038\n",
      "63  Loss: 253.00168371200562\n",
      "64  Loss: 256.9980607032776\n",
      "65  Loss: 260.9740734100342\n",
      "66  Loss: 264.84031653404236\n",
      "67  Loss: 268.70460748672485\n",
      "68  Loss: 272.6560516357422\n",
      "69  Loss: 276.6642827987671\n",
      "70  Loss: 280.528329372406\n",
      "71  Loss: 284.48713970184326\n",
      "72  Loss: 288.4262933731079\n",
      "73  Loss: 292.4273190498352\n",
      "74  Loss: 296.43305826187134\n",
      "75  Loss: 300.3761053085327\n",
      "76  Loss: 304.3517813682556\n",
      "77  Loss: 308.2365288734436\n",
      "78  Loss: 312.2212746143341\n",
      "79  Loss: 316.1516191959381\n",
      "80  Loss: 320.1128134727478\n",
      "81  Loss: 323.96301221847534\n",
      "82  Loss: 327.81358671188354\n",
      "83  Loss: 331.7929286956787\n",
      "84  Loss: 335.682989358902\n",
      "85  Loss: 339.6873023509979\n",
      "86  Loss: 343.69502997398376\n",
      "87  Loss: 347.7554409503937\n",
      "88  Loss: 351.6861448287964\n",
      "89  Loss: 355.6360242366791\n",
      "90  Loss: 359.5089273452759\n",
      "91  Loss: 363.49502968788147\n",
      "92  Loss: 367.4597406387329\n",
      "93  Loss: 371.4335572719574\n",
      "Epoch [12/20], Loss: 3.9514\n",
      "0  Loss: 4.071276664733887\n",
      "1  Loss: 8.047539234161377\n",
      "2  Loss: 11.97354245185852\n",
      "3  Loss: 15.895879983901978\n",
      "4  Loss: 19.751644372940063\n",
      "5  Loss: 23.60588312149048\n",
      "6  Loss: 27.474242210388184\n",
      "7  Loss: 31.41469097137451\n",
      "8  Loss: 35.37999439239502\n",
      "9  Loss: 39.368934631347656\n",
      "10  Loss: 43.26064872741699\n",
      "11  Loss: 47.17838406562805\n",
      "12  Loss: 51.150251626968384\n",
      "13  Loss: 55.02054286003113\n",
      "14  Loss: 58.93123698234558\n",
      "15  Loss: 62.88902282714844\n",
      "16  Loss: 66.83777213096619\n",
      "17  Loss: 70.78443121910095\n",
      "18  Loss: 74.7440128326416\n",
      "19  Loss: 78.70892477035522\n",
      "20  Loss: 82.72096014022827\n",
      "21  Loss: 86.56499218940735\n",
      "22  Loss: 90.5088210105896\n",
      "23  Loss: 94.4592695236206\n",
      "24  Loss: 98.39235258102417\n",
      "25  Loss: 102.35484051704407\n",
      "26  Loss: 106.35681366920471\n",
      "27  Loss: 110.22560787200928\n",
      "28  Loss: 114.22484493255615\n",
      "29  Loss: 118.17027473449707\n",
      "30  Loss: 122.16153955459595\n",
      "31  Loss: 126.09635257720947\n",
      "32  Loss: 130.15813827514648\n",
      "33  Loss: 133.97047352790833\n",
      "34  Loss: 137.8887460231781\n",
      "35  Loss: 141.8273983001709\n",
      "36  Loss: 145.79706048965454\n",
      "37  Loss: 149.79496574401855\n",
      "38  Loss: 153.80141639709473\n",
      "39  Loss: 157.7042601108551\n",
      "40  Loss: 161.65491890907288\n",
      "41  Loss: 165.62169122695923\n",
      "42  Loss: 169.53133869171143\n",
      "43  Loss: 173.42295479774475\n",
      "44  Loss: 177.29594945907593\n",
      "45  Loss: 181.26950812339783\n",
      "46  Loss: 185.21468663215637\n",
      "47  Loss: 189.07921528816223\n",
      "48  Loss: 193.02506494522095\n",
      "49  Loss: 196.97352719306946\n",
      "50  Loss: 200.79975533485413\n",
      "51  Loss: 204.80841898918152\n",
      "52  Loss: 208.68639469146729\n",
      "53  Loss: 212.65195727348328\n",
      "54  Loss: 216.52212524414062\n",
      "55  Loss: 220.39592623710632\n",
      "56  Loss: 224.4181191921234\n",
      "57  Loss: 228.44486689567566\n",
      "58  Loss: 232.43651938438416\n",
      "59  Loss: 236.24103927612305\n",
      "60  Loss: 240.24017643928528\n",
      "61  Loss: 244.20185375213623\n",
      "62  Loss: 248.1086688041687\n",
      "63  Loss: 252.08713245391846\n",
      "64  Loss: 256.02335476875305\n",
      "65  Loss: 260.0412428379059\n",
      "66  Loss: 264.0680868625641\n",
      "67  Loss: 267.92076992988586\n",
      "68  Loss: 271.76068139076233\n",
      "69  Loss: 275.7252547740936\n",
      "70  Loss: 279.5858647823334\n",
      "71  Loss: 283.420893907547\n",
      "72  Loss: 287.42855048179626\n",
      "73  Loss: 291.5039794445038\n",
      "74  Loss: 295.5547454357147\n",
      "75  Loss: 299.5539605617523\n",
      "76  Loss: 303.5484709739685\n",
      "77  Loss: 307.5439569950104\n",
      "78  Loss: 311.41579031944275\n",
      "79  Loss: 315.34402775764465\n",
      "80  Loss: 319.3750283718109\n",
      "81  Loss: 323.1590654850006\n",
      "82  Loss: 327.21270060539246\n",
      "83  Loss: 331.1214165687561\n",
      "84  Loss: 335.18301248550415\n",
      "85  Loss: 339.1745014190674\n",
      "86  Loss: 343.07057428359985\n",
      "87  Loss: 347.09400939941406\n",
      "88  Loss: 351.0323143005371\n",
      "89  Loss: 355.00961112976074\n",
      "90  Loss: 358.964506149292\n",
      "91  Loss: 362.9741907119751\n",
      "92  Loss: 366.9354844093323\n",
      "93  Loss: 370.78173327445984\n",
      "Epoch [13/20], Loss: 3.9445\n",
      "0  Loss: 3.9919145107269287\n",
      "1  Loss: 8.000050783157349\n",
      "2  Loss: 11.885233163833618\n",
      "3  Loss: 15.740075588226318\n",
      "4  Loss: 19.78088665008545\n",
      "5  Loss: 23.703251361846924\n",
      "6  Loss: 27.59641695022583\n",
      "7  Loss: 31.47044801712036\n",
      "8  Loss: 35.49283742904663\n",
      "9  Loss: 39.270755767822266\n",
      "10  Loss: 43.2327663898468\n",
      "11  Loss: 47.26118063926697\n",
      "12  Loss: 51.225953578948975\n",
      "13  Loss: 55.34385919570923\n",
      "14  Loss: 59.38279104232788\n",
      "15  Loss: 63.30730843544006\n",
      "16  Loss: 67.13105940818787\n",
      "17  Loss: 71.02702236175537\n",
      "18  Loss: 74.9715633392334\n",
      "19  Loss: 78.94615459442139\n",
      "20  Loss: 82.89228987693787\n",
      "21  Loss: 86.85659384727478\n",
      "22  Loss: 90.67690706253052\n",
      "23  Loss: 94.56286907196045\n",
      "24  Loss: 98.4889395236969\n",
      "25  Loss: 102.4406590461731\n",
      "26  Loss: 106.41265821456909\n",
      "27  Loss: 110.26545786857605\n",
      "28  Loss: 114.22990012168884\n",
      "29  Loss: 118.13544750213623\n",
      "30  Loss: 122.17429161071777\n",
      "31  Loss: 126.16089081764221\n",
      "32  Loss: 130.06220483779907\n",
      "33  Loss: 134.09676218032837\n",
      "34  Loss: 138.0878643989563\n",
      "35  Loss: 142.09466552734375\n",
      "36  Loss: 145.90635776519775\n",
      "37  Loss: 149.8208875656128\n",
      "38  Loss: 153.87213325500488\n",
      "39  Loss: 157.73702216148376\n",
      "40  Loss: 161.79879546165466\n",
      "41  Loss: 165.74134874343872\n",
      "42  Loss: 169.67322373390198\n",
      "43  Loss: 173.69261622428894\n",
      "44  Loss: 177.63666653633118\n",
      "45  Loss: 181.60059332847595\n",
      "46  Loss: 185.49043583869934\n",
      "47  Loss: 189.45975756645203\n",
      "48  Loss: 193.2912323474884\n",
      "49  Loss: 197.17669439315796\n",
      "50  Loss: 201.0215449333191\n",
      "51  Loss: 204.9025890827179\n",
      "52  Loss: 208.9253203868866\n",
      "53  Loss: 212.91785216331482\n",
      "54  Loss: 216.8306601047516\n",
      "55  Loss: 220.6657567024231\n",
      "56  Loss: 224.73018169403076\n",
      "57  Loss: 228.68723726272583\n",
      "58  Loss: 232.61469674110413\n",
      "59  Loss: 236.5057511329651\n",
      "60  Loss: 240.41074132919312\n",
      "61  Loss: 244.33985996246338\n",
      "62  Loss: 248.16361546516418\n",
      "63  Loss: 252.13917875289917\n",
      "64  Loss: 256.1102707386017\n",
      "65  Loss: 259.97710490226746\n",
      "66  Loss: 263.8997972011566\n",
      "67  Loss: 267.87235975265503\n",
      "68  Loss: 271.78592324256897\n",
      "69  Loss: 275.6360263824463\n",
      "70  Loss: 279.4891355037689\n",
      "71  Loss: 283.497376203537\n",
      "72  Loss: 287.4614624977112\n",
      "73  Loss: 291.36915159225464\n",
      "74  Loss: 295.31168389320374\n",
      "75  Loss: 299.2795798778534\n",
      "76  Loss: 303.337687253952\n",
      "77  Loss: 307.39507269859314\n",
      "78  Loss: 311.34694361686707\n",
      "79  Loss: 315.21821999549866\n",
      "80  Loss: 319.1118309497833\n",
      "81  Loss: 323.09271025657654\n",
      "82  Loss: 327.0735459327698\n",
      "83  Loss: 331.0022792816162\n",
      "84  Loss: 334.9853525161743\n",
      "85  Loss: 338.8428862094879\n",
      "86  Loss: 342.8774492740631\n",
      "87  Loss: 346.8699474334717\n",
      "88  Loss: 350.81846928596497\n",
      "89  Loss: 354.83419251441956\n",
      "90  Loss: 358.7604892253876\n",
      "91  Loss: 362.6731188297272\n",
      "92  Loss: 366.4448997974396\n",
      "93  Loss: 370.4175901412964\n",
      "Epoch [14/20], Loss: 3.9406\n",
      "0  Loss: 3.9315950870513916\n",
      "1  Loss: 7.926760673522949\n",
      "2  Loss: 11.85182785987854\n",
      "3  Loss: 15.823877573013306\n",
      "4  Loss: 19.787763833999634\n",
      "5  Loss: 23.70647144317627\n",
      "6  Loss: 27.617513179779053\n",
      "7  Loss: 31.36780285835266\n",
      "8  Loss: 35.21726632118225\n",
      "9  Loss: 39.0224289894104\n",
      "10  Loss: 42.9378719329834\n",
      "11  Loss: 46.86722111701965\n",
      "12  Loss: 50.87928557395935\n",
      "13  Loss: 54.843488931655884\n",
      "14  Loss: 58.77661728858948\n",
      "15  Loss: 62.7490508556366\n",
      "16  Loss: 66.61956310272217\n",
      "17  Loss: 70.43693113327026\n",
      "18  Loss: 74.38939380645752\n",
      "19  Loss: 78.24784278869629\n",
      "20  Loss: 82.0522050857544\n",
      "21  Loss: 85.9861798286438\n",
      "22  Loss: 89.89740490913391\n",
      "23  Loss: 93.72197318077087\n",
      "24  Loss: 97.67438769340515\n",
      "25  Loss: 101.55924439430237\n",
      "26  Loss: 105.51499700546265\n",
      "27  Loss: 109.40801286697388\n",
      "28  Loss: 113.46062469482422\n",
      "29  Loss: 117.48651075363159\n",
      "30  Loss: 121.51295709609985\n",
      "31  Loss: 125.45781016349792\n",
      "32  Loss: 129.4776074886322\n",
      "33  Loss: 133.36030292510986\n",
      "34  Loss: 137.2947907447815\n",
      "35  Loss: 141.38822841644287\n",
      "36  Loss: 145.36939930915833\n",
      "37  Loss: 149.23417973518372\n",
      "38  Loss: 153.24493861198425\n",
      "39  Loss: 157.16043639183044\n",
      "40  Loss: 161.04372715950012\n",
      "41  Loss: 164.95938515663147\n",
      "42  Loss: 168.89790415763855\n",
      "43  Loss: 172.85581970214844\n",
      "44  Loss: 176.75342059135437\n",
      "45  Loss: 180.71195483207703\n",
      "46  Loss: 184.7220914363861\n",
      "47  Loss: 188.71041059494019\n",
      "48  Loss: 192.64636898040771\n",
      "49  Loss: 196.64046931266785\n",
      "50  Loss: 200.57611274719238\n",
      "51  Loss: 204.55791354179382\n",
      "52  Loss: 208.58034539222717\n",
      "53  Loss: 212.57750344276428\n",
      "54  Loss: 216.5055387020111\n",
      "55  Loss: 220.5272786617279\n",
      "56  Loss: 224.43069505691528\n",
      "57  Loss: 228.4364423751831\n",
      "58  Loss: 232.43026995658875\n",
      "59  Loss: 236.309143781662\n",
      "60  Loss: 240.31298661231995\n",
      "61  Loss: 244.2042031288147\n",
      "62  Loss: 248.12609386444092\n",
      "63  Loss: 252.1286849975586\n",
      "64  Loss: 256.0977613925934\n",
      "65  Loss: 260.028751373291\n",
      "66  Loss: 263.90536403656006\n",
      "67  Loss: 267.8557324409485\n",
      "68  Loss: 271.7329969406128\n",
      "69  Loss: 275.56057834625244\n",
      "70  Loss: 279.5416450500488\n",
      "71  Loss: 283.47482919692993\n",
      "72  Loss: 287.3939456939697\n",
      "73  Loss: 291.3305287361145\n",
      "74  Loss: 295.2911763191223\n",
      "75  Loss: 299.09178614616394\n",
      "76  Loss: 303.11622309684753\n",
      "77  Loss: 307.0652651786804\n",
      "78  Loss: 311.01909136772156\n",
      "79  Loss: 314.98364877700806\n",
      "80  Loss: 318.9184353351593\n",
      "81  Loss: 322.793847322464\n",
      "82  Loss: 326.72194623947144\n",
      "83  Loss: 330.62704396247864\n",
      "84  Loss: 334.61520314216614\n",
      "85  Loss: 338.65365958213806\n",
      "86  Loss: 342.59778690338135\n",
      "87  Loss: 346.50715923309326\n",
      "88  Loss: 350.46243047714233\n",
      "89  Loss: 354.34167766571045\n",
      "90  Loss: 358.2946891784668\n",
      "91  Loss: 362.22768568992615\n",
      "92  Loss: 366.1786847114563\n",
      "93  Loss: 370.13766074180603\n",
      "Epoch [15/20], Loss: 3.9376\n",
      "0  Loss: 3.9530210494995117\n",
      "1  Loss: 7.8997461795806885\n",
      "2  Loss: 11.835604667663574\n",
      "3  Loss: 15.829585790634155\n",
      "4  Loss: 19.77060890197754\n",
      "5  Loss: 23.704723358154297\n",
      "6  Loss: 27.616336822509766\n",
      "7  Loss: 31.59092926979065\n",
      "8  Loss: 35.47484040260315\n",
      "9  Loss: 39.34662055969238\n",
      "10  Loss: 43.34835338592529\n",
      "11  Loss: 47.249815225601196\n",
      "12  Loss: 51.012768030166626\n",
      "13  Loss: 54.762290477752686\n",
      "14  Loss: 58.682124614715576\n",
      "15  Loss: 62.657546043395996\n",
      "16  Loss: 66.59829711914062\n",
      "17  Loss: 70.52629375457764\n",
      "18  Loss: 74.50950360298157\n",
      "19  Loss: 78.40836954116821\n",
      "20  Loss: 82.37498116493225\n",
      "21  Loss: 86.43798089027405\n",
      "22  Loss: 90.2931125164032\n",
      "23  Loss: 94.22316002845764\n",
      "24  Loss: 98.22407221794128\n",
      "25  Loss: 102.18160080909729\n",
      "26  Loss: 106.04776263237\n",
      "27  Loss: 110.03198075294495\n",
      "28  Loss: 113.99523186683655\n",
      "29  Loss: 117.89568018913269\n",
      "30  Loss: 121.73388457298279\n",
      "31  Loss: 125.61605906486511\n",
      "32  Loss: 129.55502128601074\n",
      "33  Loss: 133.54883742332458\n",
      "34  Loss: 137.48421478271484\n",
      "35  Loss: 141.39906644821167\n",
      "36  Loss: 145.36418557167053\n",
      "37  Loss: 149.2272744178772\n",
      "38  Loss: 153.19500494003296\n",
      "39  Loss: 157.09052848815918\n",
      "40  Loss: 161.07799124717712\n",
      "41  Loss: 165.0325529575348\n",
      "42  Loss: 168.99052214622498\n",
      "43  Loss: 173.02819657325745\n",
      "44  Loss: 177.00455403327942\n",
      "45  Loss: 180.96447587013245\n",
      "46  Loss: 184.98599982261658\n",
      "47  Loss: 188.91677618026733\n",
      "48  Loss: 192.91863346099854\n",
      "49  Loss: 196.82997488975525\n",
      "50  Loss: 200.7945351600647\n",
      "51  Loss: 204.63918828964233\n",
      "52  Loss: 208.61499857902527\n",
      "53  Loss: 212.58029294013977\n",
      "54  Loss: 216.5265338420868\n",
      "55  Loss: 220.45713901519775\n",
      "56  Loss: 224.42051434516907\n",
      "57  Loss: 228.40829944610596\n",
      "58  Loss: 232.4197473526001\n",
      "59  Loss: 236.42006921768188\n",
      "60  Loss: 240.39776611328125\n",
      "61  Loss: 244.40800142288208\n",
      "62  Loss: 248.2981824874878\n",
      "63  Loss: 252.29729437828064\n",
      "64  Loss: 256.2173581123352\n",
      "65  Loss: 260.1563060283661\n",
      "66  Loss: 264.0812349319458\n",
      "67  Loss: 267.8992483615875\n",
      "68  Loss: 271.72241258621216\n",
      "69  Loss: 275.60300397872925\n",
      "70  Loss: 279.4697265625\n",
      "71  Loss: 283.2903981208801\n",
      "72  Loss: 287.2810654640198\n",
      "73  Loss: 291.1825523376465\n",
      "74  Loss: 295.0714681148529\n",
      "75  Loss: 299.1133258342743\n",
      "76  Loss: 303.02665424346924\n",
      "77  Loss: 306.886522769928\n",
      "78  Loss: 310.7865056991577\n",
      "79  Loss: 314.8235206604004\n",
      "80  Loss: 318.66794896125793\n",
      "81  Loss: 322.6325545310974\n",
      "82  Loss: 326.53144240379333\n",
      "83  Loss: 330.42407989501953\n",
      "84  Loss: 334.3734040260315\n",
      "85  Loss: 338.3070366382599\n",
      "86  Loss: 342.2647182941437\n",
      "87  Loss: 346.22716784477234\n",
      "88  Loss: 350.22215509414673\n",
      "89  Loss: 354.2477960586548\n",
      "90  Loss: 358.21299386024475\n",
      "91  Loss: 362.1757755279541\n",
      "92  Loss: 366.11085772514343\n",
      "93  Loss: 370.00615072250366\n",
      "Epoch [16/20], Loss: 3.9362\n",
      "0  Loss: 3.9445059299468994\n",
      "1  Loss: 7.921864986419678\n",
      "2  Loss: 11.94986867904663\n",
      "3  Loss: 15.792829275131226\n",
      "4  Loss: 19.70460343360901\n",
      "5  Loss: 23.561645984649658\n",
      "6  Loss: 27.543349266052246\n",
      "7  Loss: 31.403532028198242\n",
      "8  Loss: 35.39565587043762\n",
      "9  Loss: 39.210755348205566\n",
      "10  Loss: 43.20300364494324\n",
      "11  Loss: 47.20842623710632\n",
      "12  Loss: 51.148640155792236\n",
      "13  Loss: 55.0996458530426\n",
      "14  Loss: 58.99944281578064\n",
      "15  Loss: 62.859216928482056\n",
      "16  Loss: 66.82671689987183\n",
      "17  Loss: 70.82586669921875\n",
      "18  Loss: 74.77328944206238\n",
      "19  Loss: 78.74921345710754\n",
      "20  Loss: 82.6802008152008\n",
      "21  Loss: 86.70577311515808\n",
      "22  Loss: 90.61232733726501\n",
      "23  Loss: 94.57452869415283\n",
      "24  Loss: 98.50670218467712\n",
      "25  Loss: 102.46078324317932\n",
      "26  Loss: 106.41161894798279\n",
      "27  Loss: 110.3676688671112\n",
      "28  Loss: 114.34441781044006\n",
      "29  Loss: 118.39236569404602\n",
      "30  Loss: 122.3581292629242\n",
      "31  Loss: 126.3524534702301\n",
      "32  Loss: 130.28289031982422\n",
      "33  Loss: 134.26383209228516\n",
      "34  Loss: 138.21997714042664\n",
      "35  Loss: 142.00048828125\n",
      "36  Loss: 145.78866744041443\n",
      "37  Loss: 149.66159844398499\n",
      "38  Loss: 153.5429651737213\n",
      "39  Loss: 157.51525497436523\n",
      "40  Loss: 161.43615984916687\n",
      "41  Loss: 165.37775874137878\n",
      "42  Loss: 169.24825811386108\n",
      "43  Loss: 173.15957283973694\n",
      "44  Loss: 177.12171506881714\n",
      "45  Loss: 181.01509189605713\n",
      "46  Loss: 184.82576942443848\n",
      "47  Loss: 188.72539377212524\n",
      "48  Loss: 192.7001621723175\n",
      "49  Loss: 196.65778827667236\n",
      "50  Loss: 200.58301830291748\n",
      "51  Loss: 204.50171327590942\n",
      "52  Loss: 208.4300856590271\n",
      "53  Loss: 212.45339393615723\n",
      "54  Loss: 216.45862245559692\n",
      "55  Loss: 220.2350709438324\n",
      "56  Loss: 224.15040183067322\n",
      "57  Loss: 228.12408185005188\n",
      "58  Loss: 232.1295621395111\n",
      "59  Loss: 236.08662247657776\n",
      "60  Loss: 240.11192202568054\n",
      "61  Loss: 244.06823015213013\n",
      "62  Loss: 248.0294210910797\n",
      "63  Loss: 252.02357697486877\n",
      "64  Loss: 255.92457342147827\n",
      "65  Loss: 259.8230359554291\n",
      "66  Loss: 263.6786377429962\n",
      "67  Loss: 267.6283321380615\n",
      "68  Loss: 271.5712444782257\n",
      "69  Loss: 275.5250041484833\n",
      "70  Loss: 279.4286003112793\n",
      "71  Loss: 283.4244956970215\n",
      "72  Loss: 287.455828666687\n",
      "73  Loss: 291.4003086090088\n",
      "74  Loss: 295.419442653656\n",
      "75  Loss: 299.1762456893921\n",
      "76  Loss: 303.1720781326294\n",
      "77  Loss: 307.04963207244873\n",
      "78  Loss: 311.0455918312073\n",
      "79  Loss: 314.9838316440582\n",
      "80  Loss: 318.88436102867126\n",
      "81  Loss: 322.83491253852844\n",
      "82  Loss: 326.76628398895264\n",
      "83  Loss: 330.63392758369446\n",
      "84  Loss: 334.6008117198944\n",
      "85  Loss: 338.3847322463989\n",
      "86  Loss: 342.178368806839\n",
      "87  Loss: 346.15405917167664\n",
      "88  Loss: 350.0780761241913\n",
      "89  Loss: 353.99761605262756\n",
      "90  Loss: 357.8716378211975\n",
      "91  Loss: 361.77759289741516\n",
      "92  Loss: 365.7565276622772\n",
      "93  Loss: 369.79833722114563\n",
      "Epoch [17/20], Loss: 3.9340\n",
      "0  Loss: 3.9587502479553223\n",
      "1  Loss: 7.8938281536102295\n",
      "2  Loss: 11.892014980316162\n",
      "3  Loss: 15.768466711044312\n",
      "4  Loss: 19.841442346572876\n",
      "5  Loss: 23.868801832199097\n",
      "6  Loss: 27.660871505737305\n",
      "7  Loss: 31.66639471054077\n",
      "8  Loss: 35.63965916633606\n",
      "9  Loss: 39.60084390640259\n",
      "10  Loss: 43.628498554229736\n",
      "11  Loss: 47.47895526885986\n",
      "12  Loss: 51.534531593322754\n",
      "13  Loss: 55.40587830543518\n",
      "14  Loss: 59.28416037559509\n",
      "15  Loss: 63.18166542053223\n",
      "16  Loss: 67.1857852935791\n",
      "17  Loss: 71.05327582359314\n",
      "18  Loss: 75.04193449020386\n",
      "19  Loss: 78.859543800354\n",
      "20  Loss: 82.78804445266724\n",
      "21  Loss: 86.78793692588806\n",
      "22  Loss: 90.6627368927002\n",
      "23  Loss: 94.56711196899414\n",
      "24  Loss: 98.46405982971191\n",
      "25  Loss: 102.37259697914124\n",
      "26  Loss: 106.22899580001831\n",
      "27  Loss: 110.1331729888916\n",
      "28  Loss: 114.12884712219238\n",
      "29  Loss: 118.12012505531311\n",
      "30  Loss: 122.09668469429016\n",
      "31  Loss: 126.02896451950073\n",
      "32  Loss: 129.9427318572998\n",
      "33  Loss: 133.85986351966858\n",
      "34  Loss: 137.7486617565155\n",
      "35  Loss: 141.61268377304077\n",
      "36  Loss: 145.47498035430908\n",
      "37  Loss: 149.40324592590332\n",
      "38  Loss: 153.3349575996399\n",
      "39  Loss: 157.33647632598877\n",
      "40  Loss: 161.2906618118286\n",
      "41  Loss: 165.23041415214539\n",
      "42  Loss: 169.14795351028442\n",
      "43  Loss: 173.10382199287415\n",
      "44  Loss: 177.03793454170227\n",
      "45  Loss: 181.03968739509583\n",
      "46  Loss: 185.0190875530243\n",
      "47  Loss: 188.89833307266235\n",
      "48  Loss: 192.823548078537\n",
      "49  Loss: 196.72825717926025\n",
      "50  Loss: 200.73228073120117\n",
      "51  Loss: 204.61401748657227\n",
      "52  Loss: 208.58924746513367\n",
      "53  Loss: 212.48972129821777\n",
      "54  Loss: 216.39387226104736\n",
      "55  Loss: 220.22638249397278\n",
      "56  Loss: 224.16895747184753\n",
      "57  Loss: 228.03500699996948\n",
      "58  Loss: 232.03994131088257\n",
      "59  Loss: 236.02264618873596\n",
      "60  Loss: 239.9661123752594\n",
      "61  Loss: 243.82245469093323\n",
      "62  Loss: 247.76819896697998\n",
      "63  Loss: 251.68048429489136\n",
      "64  Loss: 255.61410975456238\n",
      "65  Loss: 259.54824590682983\n",
      "66  Loss: 263.51040029525757\n",
      "67  Loss: 267.452232837677\n",
      "68  Loss: 271.3197510242462\n",
      "69  Loss: 275.2804226875305\n",
      "70  Loss: 279.2330777645111\n",
      "71  Loss: 283.1563844680786\n",
      "72  Loss: 287.0434744358063\n",
      "73  Loss: 290.9322967529297\n",
      "74  Loss: 294.7500286102295\n",
      "75  Loss: 298.71418952941895\n",
      "76  Loss: 302.64474654197693\n",
      "77  Loss: 306.54341554641724\n",
      "78  Loss: 310.5171504020691\n",
      "79  Loss: 314.3031876087189\n",
      "80  Loss: 318.1679356098175\n",
      "81  Loss: 322.03884530067444\n",
      "82  Loss: 326.01710319519043\n",
      "83  Loss: 329.93995213508606\n",
      "84  Loss: 333.9376571178436\n",
      "85  Loss: 337.8392605781555\n",
      "86  Loss: 341.74410605430603\n",
      "87  Loss: 345.5580816268921\n",
      "88  Loss: 349.4709463119507\n",
      "89  Loss: 353.3944127559662\n",
      "90  Loss: 357.3769545555115\n",
      "91  Loss: 361.3292751312256\n",
      "92  Loss: 365.3142080307007\n",
      "93  Loss: 369.2496569156647\n",
      "Epoch [18/20], Loss: 3.9282\n",
      "0  Loss: 3.9715964794158936\n",
      "1  Loss: 7.855778217315674\n",
      "2  Loss: 11.759601831436157\n",
      "3  Loss: 15.60007357597351\n",
      "4  Loss: 19.5315523147583\n",
      "5  Loss: 23.464395999908447\n",
      "6  Loss: 27.400569677352905\n",
      "7  Loss: 31.368245601654053\n",
      "8  Loss: 35.31328082084656\n",
      "9  Loss: 39.16004514694214\n",
      "10  Loss: 43.06702995300293\n",
      "11  Loss: 47.00075316429138\n",
      "12  Loss: 50.939661741256714\n",
      "13  Loss: 54.879722356796265\n",
      "14  Loss: 58.86991477012634\n",
      "15  Loss: 62.89396643638611\n",
      "16  Loss: 66.80001735687256\n",
      "17  Loss: 70.81845617294312\n",
      "18  Loss: 74.75400042533875\n",
      "19  Loss: 78.73388338088989\n",
      "20  Loss: 82.54321622848511\n",
      "21  Loss: 86.5278947353363\n",
      "22  Loss: 90.480144739151\n",
      "23  Loss: 94.4312973022461\n",
      "24  Loss: 98.33662939071655\n",
      "25  Loss: 102.19898653030396\n",
      "26  Loss: 106.18037056922913\n",
      "27  Loss: 110.0279381275177\n",
      "28  Loss: 113.9648129940033\n",
      "29  Loss: 117.87060952186584\n",
      "30  Loss: 121.81662082672119\n",
      "31  Loss: 125.76655101776123\n",
      "32  Loss: 129.7074339389801\n",
      "33  Loss: 133.73075985908508\n",
      "34  Loss: 137.7663996219635\n",
      "35  Loss: 141.69537234306335\n",
      "36  Loss: 145.70128989219666\n",
      "37  Loss: 149.74360156059265\n",
      "38  Loss: 153.73558020591736\n",
      "39  Loss: 157.51383113861084\n",
      "40  Loss: 161.51664018630981\n",
      "41  Loss: 165.47270250320435\n",
      "42  Loss: 169.43503499031067\n",
      "43  Loss: 173.28154516220093\n",
      "44  Loss: 177.2622892856598\n",
      "45  Loss: 181.11722660064697\n",
      "46  Loss: 185.0190680027008\n",
      "47  Loss: 189.01258611679077\n",
      "48  Loss: 193.07428979873657\n",
      "49  Loss: 197.1124119758606\n",
      "50  Loss: 200.95324540138245\n",
      "51  Loss: 204.89086771011353\n",
      "52  Loss: 208.79012656211853\n",
      "53  Loss: 212.48172068595886\n",
      "54  Loss: 216.3709008693695\n",
      "55  Loss: 220.34190797805786\n",
      "56  Loss: 224.3052101135254\n",
      "57  Loss: 228.11960220336914\n",
      "58  Loss: 232.07549214363098\n",
      "59  Loss: 236.13618111610413\n",
      "60  Loss: 239.952219247818\n",
      "61  Loss: 243.8145308494568\n",
      "62  Loss: 247.7525510787964\n",
      "63  Loss: 251.6403169631958\n",
      "64  Loss: 255.55972909927368\n",
      "65  Loss: 259.41571402549744\n",
      "66  Loss: 263.47519516944885\n",
      "67  Loss: 267.34629225730896\n",
      "68  Loss: 271.36046290397644\n",
      "69  Loss: 275.2125084400177\n",
      "70  Loss: 279.14331436157227\n",
      "71  Loss: 283.0815050601959\n",
      "72  Loss: 286.9792809486389\n",
      "73  Loss: 290.8991503715515\n",
      "74  Loss: 294.71438241004944\n",
      "75  Loss: 298.51073575019836\n",
      "76  Loss: 302.28502368927\n",
      "77  Loss: 306.1670882701874\n",
      "78  Loss: 310.12829780578613\n",
      "79  Loss: 313.9905812740326\n",
      "80  Loss: 317.83132314682007\n",
      "81  Loss: 321.65017342567444\n",
      "82  Loss: 325.38892579078674\n",
      "83  Loss: 329.4151728153229\n",
      "84  Loss: 333.348130941391\n",
      "85  Loss: 337.17481875419617\n",
      "86  Loss: 341.04086232185364\n",
      "87  Loss: 344.8544261455536\n",
      "88  Loss: 348.7807648181915\n",
      "89  Loss: 352.8015811443329\n",
      "90  Loss: 356.6677396297455\n",
      "91  Loss: 360.528044462204\n",
      "92  Loss: 364.4899094104767\n",
      "93  Loss: 368.3725073337555\n",
      "Epoch [19/20], Loss: 3.9189\n",
      "0  Loss: 3.8920955657958984\n",
      "1  Loss: 7.7255518436431885\n",
      "2  Loss: 11.683610439300537\n",
      "3  Loss: 15.510922908782959\n",
      "4  Loss: 19.44306778907776\n",
      "5  Loss: 23.405591249465942\n",
      "6  Loss: 27.430608987808228\n",
      "7  Loss: 31.312578678131104\n",
      "8  Loss: 35.163737773895264\n",
      "9  Loss: 39.05074667930603\n",
      "10  Loss: 43.0388720035553\n",
      "11  Loss: 46.90999627113342\n",
      "12  Loss: 50.779125928878784\n",
      "13  Loss: 54.72311520576477\n",
      "14  Loss: 58.68747162818909\n",
      "15  Loss: 62.557968854904175\n",
      "16  Loss: 66.5324592590332\n",
      "17  Loss: 70.3662941455841\n",
      "18  Loss: 74.29423260688782\n",
      "19  Loss: 78.08413195610046\n",
      "20  Loss: 81.98477554321289\n",
      "21  Loss: 85.97799181938171\n",
      "22  Loss: 89.86171054840088\n",
      "23  Loss: 93.80829286575317\n",
      "24  Loss: 97.62416338920593\n",
      "25  Loss: 101.58813977241516\n",
      "26  Loss: 105.56581139564514\n",
      "27  Loss: 109.43291854858398\n",
      "28  Loss: 113.37039232254028\n",
      "29  Loss: 117.34409236907959\n",
      "30  Loss: 121.22576880455017\n",
      "31  Loss: 125.16314840316772\n",
      "32  Loss: 129.05956888198853\n",
      "33  Loss: 132.95836901664734\n",
      "34  Loss: 136.920423746109\n",
      "35  Loss: 140.70946669578552\n",
      "36  Loss: 144.64185214042664\n",
      "37  Loss: 148.51117730140686\n",
      "38  Loss: 152.45790934562683\n",
      "39  Loss: 156.3972201347351\n",
      "40  Loss: 160.38061094284058\n",
      "41  Loss: 164.3699779510498\n",
      "42  Loss: 168.2541699409485\n",
      "43  Loss: 172.1889786720276\n",
      "44  Loss: 175.97110795974731\n",
      "45  Loss: 179.7650969028473\n",
      "46  Loss: 183.6699414253235\n",
      "47  Loss: 187.6338987350464\n",
      "48  Loss: 191.5639853477478\n",
      "49  Loss: 195.39672541618347\n",
      "50  Loss: 199.3769030570984\n",
      "51  Loss: 203.31080555915833\n",
      "52  Loss: 207.24493646621704\n",
      "53  Loss: 211.17860198020935\n",
      "54  Loss: 214.97369003295898\n",
      "55  Loss: 218.9546082019806\n",
      "56  Loss: 222.92076539993286\n",
      "57  Loss: 226.9612193107605\n",
      "58  Loss: 230.8105342388153\n",
      "59  Loss: 234.81308245658875\n",
      "60  Loss: 238.63336873054504\n",
      "61  Loss: 242.6100947856903\n",
      "62  Loss: 246.56004214286804\n",
      "63  Loss: 250.45337557792664\n",
      "64  Loss: 254.3936469554901\n",
      "65  Loss: 258.2721984386444\n",
      "66  Loss: 262.2191185951233\n",
      "67  Loss: 266.1541836261749\n",
      "68  Loss: 270.09578108787537\n",
      "69  Loss: 273.97658824920654\n",
      "70  Loss: 277.84084963798523\n",
      "71  Loss: 281.7543001174927\n",
      "72  Loss: 285.7176971435547\n",
      "73  Loss: 289.68695974349976\n",
      "74  Loss: 293.6830041408539\n",
      "75  Loss: 297.4573483467102\n",
      "76  Loss: 301.38884353637695\n",
      "77  Loss: 305.3647794723511\n",
      "78  Loss: 309.32820558547974\n",
      "79  Loss: 313.241491317749\n",
      "80  Loss: 317.2292182445526\n",
      "81  Loss: 320.92457818984985\n",
      "82  Loss: 324.876051902771\n",
      "83  Loss: 328.8176109790802\n",
      "84  Loss: 332.79039454460144\n",
      "85  Loss: 336.80154728889465\n",
      "86  Loss: 340.70228338241577\n",
      "87  Loss: 344.5332136154175\n",
      "88  Loss: 348.3660719394684\n",
      "89  Loss: 352.3244638442993\n",
      "90  Loss: 356.2671902179718\n",
      "91  Loss: 360.2604355812073\n",
      "92  Loss: 364.17925572395325\n",
      "93  Loss: 368.0866813659668\n",
      "Epoch [20/20], Loss: 3.9158\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20  # Number of epochs to train\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for images, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        labels = labels.long()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "        print(i , \" Loss:\",running_loss)\n",
    "        i+=1\n",
    "    # Print the average loss for this epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 27.80%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, val_loader)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
